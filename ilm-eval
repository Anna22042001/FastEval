#!/usr/bin/env python3

import multiprocessing

if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn')

import os
import json
import argparse
import threading
import uuid

import evaluation.args
from evaluation import benchmarks
from evaluation.utils import changed_exit_handlers
from evaluation.models.models import unload_model, create_model, compute_model_replies

def merge_models_and_benchmarks_to_evaluate(existing_models_and_benchmarks, new_model_type, new_model_name, new_benchmarks, model_args):
    if new_model_name is None:
        return existing_models_and_benchmarks

    inserted_into_existing_entry = False
    for item in existing_models_and_benchmarks:
        if item['model_name'] != new_model_name:
            continue
        if item['model_type'] != new_model_type:
            raise # TODO: Only for now. Later we allow this.
        if item['model_args'] != model_args:
            raise # TODO: Only for now. Later we allow this.
        for benchmark in new_benchmarks:
            if benchmark not in item['benchmarks']:
                item['benchmarks'].insert(0, benchmark)
        inserted_into_existing_entry = True

    if inserted_into_existing_entry:
        return existing_models_and_benchmarks

    existing_models_and_benchmarks.insert(0, {
        'id': str(uuid.uuid4()),
        'model_type': new_model_type,
        'model_name': new_model_name,
        'benchmarks': new_benchmarks,
        'model_args': model_args,
    })

    return existing_models_and_benchmarks

def run_correctness_check(model_type, model_name, model_args):
    assert model_type is not None and model_name is not None

    # Idk. Just some conversations. Can be absolutely changed to better reflect what would be good tests
    # to confirm that different inference backends output the same text.
    conversations = [
        [('user', 'Please tell me a joke.')],
        [('user', "What's 46912984610 + 5610927469123 * 64127401823 / 123?")],
        [
            ('user', 'Why did the chicken cross the road?'),
            ('assistant', 'The classic joke answer to "Why did the chicken cross the road?" is simply, "To get to the other side." '
                'The humor lies in the fact that the listener expects a more elaborate or clever answer, '
                'but the punchline is straightforward and unexpected, resulting in a lighthearted and silly joke. '
                'The joke has become a well-known and often-used example of anti-humor, '
                'where the humor comes from the lack of a complex or unexpected punchline.'),
            ('user', 'Can you write a long essay about this topic?'),
        ],
        [('user', '^_^ </s> <|endoftext|> [PAD] [PAD][PAD] What do you think? Please answer only in emojis. ^_^')],
    ]

    conversations = [{
        'conversation': conversation,
        'temperature': 0,
    } for conversation in conversations]

    def get_outputs(backend_name, conversation, n):
        return compute_model_replies(
            create_model(model_type, model_name, model_args, max_new_tokens=50),
            [conversation] * n,
            desc=model_name + ' :: Computing replies with ' + backend_name + ' backend',
        )

    previous_force_backend = evaluation.args.cmd_arguments.force_backend
    evaluation.args.cmd_arguments.force_backend = 'hf_transformers'

    hf_transformers_model_outputs = [get_outputs('hf_transformers', conversation, 1)[0] for conversation in conversations]

    evaluation.args.cmd_arguments.force_backend = previous_force_backend

    # So basically the thing is that both vLLM as well as TGI don't actually give deterministic outputs
    # when processing multiple outputs in parallel even if temperature = 0 is used. I'm not sure about the reason,
    # but there is for example this issue here:
    #
    # https://github.com/huggingface/text-generation-inference/issues/459
    # > matrix multiplications kernels in mixed precision are not deterministic
    # > and can lead to difference in generations when the batch size increase
    #
    # So it might be something like that (especially since both vLLM and TGI do this) but idk.
    #
    # Anyway. The thing is, the difference is usually quite small and doesn't really make that much of a difference.
    # But still, we can't just check whether the results are _exactly_ the same as for HF transformers,
    # because they are often not. So in order to solve this, we do multiple inferences for the same prompt
    # using those other backends and with different batch sizes and later check whether _any_ of the results
    # are equal and accept if they are.
    #
    # We also still print all of the outputs we got, so the user can confirm the results themself.
    #
    # Also note that while disabling the parallelism is possible and we then actually do get deterministic outputs
    # from the other backends, those outputs are often not the same as what HF transformers gives us.
    # We really need to do multiple inferences in parallel and then check whether any of the outputs are equal.
    #
    # Technically speaking, this is of course incorrect. But the differences seem small enough and if the results
    # are the same at least once we at least know that they do the same kind of operations, even if the exact numbers
    # end up slightly differently sometimes. And the 20x throughput increase that vLLM & TGI provide
    # compared to plain HF transformers is just too big to be ignored due to small differences like that.
    default_backend_model_outputs = [[] for _ in conversations]
    for i, conversation in enumerate(conversations):
        for n in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 25, 31, 32, 32, 64, 100, 128, 150, 199, 257]:
            default_backend_model_outputs[i] += get_outputs('default', conversation, n)
            if hf_transformers_model_outputs[i] in default_backend_model_outputs[i]:
                break

    print('@@@@@@@@@@@@@@ START CORRECTNESS CHECK RESULTS @@@@@@@@@@@@@@')

    got_error = False
    for i in range(len(conversations)):
        hf_transformers_model_output = hf_transformers_model_outputs[i].replace('\n', '\\n').replace('\r', '\\r')
        default_backend_model_outputs_on_conversation = [output.replace('\n', '\\n').replace('\r', '\\r') for output in default_backend_model_outputs[i]]

        if hf_transformers_model_output in default_backend_model_outputs_on_conversation:
            print('OK')
        else:
            print('ERROR')
            got_error = True

        print('- HF TRANSFORMERS: ' + hf_transformers_model_output)
        default_backend_model_output_counts = {}
        for default_backend_model_output in default_backend_model_outputs_on_conversation:
            if default_backend_model_output not in default_backend_model_output_counts:
                default_backend_model_output_counts[default_backend_model_output] = 0
            default_backend_model_output_counts[default_backend_model_output] += 1

        for default_backend_model_output, count in default_backend_model_output_counts.items():
            print('- DEFAULT BACKEND [' + str(count) + 'x]: ' + default_backend_model_output)

        print()

    if got_error:
        print('ERROR: Correctness check failed!')
    else:
        print('Correctness check successful!')

    print('@@@@@@@@@@@@@@ END CORRECTNESS CHECK RESULTS @@@@@@@@@@@@@@')

    unload_model()
    join_threads()

def join_threads():
    for thread in threading.enumerate():
        if thread.daemon:
            continue

        try:
            thread.join()
        except RuntimeError as error:
            if 'cannot join current thread' in error.args[0]: # main thread
                pass
            else:
                raise

def main():
    all_benchmarks = ['mt-bench', 'cot', 'human-eval-plus', 'lm-evaluation-harness']

    parser = argparse.ArgumentParser()
    parser.add_argument('-b', '--benchmarks', choices=['all'] + all_benchmarks, nargs='*', default='all',
        help='Benchmark(s) that the model will be evaluated on')
    parser.add_argument('-t', '--model-type',
        help='Type of the model that will be evaluated. Can be an API client name (openai), a prompt template (e.g. alpaca) or `fastchat` for the fastchat backend.')
    parser.add_argument('-m', '--model-name',
        help='Name of the model that will be evaluated. Depending on the type, it can be an OpenAI model name or a path to a huggingface model.')
    parser.add_argument('--model-tokenizer',
        help='By default, the tokenizer will be the same as the model, but it can also be overwritten with this argument.')
    parser.add_argument('--model-default-system-message',
        help='The default system message of the model. Only applicable for models that use a system message and only if no other system message has been specified.')
    parser.add_argument('--force-backend', choices=['vllm', 'tgi', 'hf_transformers'], required=False,
        help='Force a specific backend for model inference. By default, the backend will be selected automatically depending on model support, '
            + 'but if you encounter bugs with this you can overwrite the backend with this argument.')
    parser.add_argument('--tgi-max-batch-total-tokens', type=int, default=None)
    parser.add_argument('--run-correctness-check', action='store_true')
    args = parser.parse_args()

    evaluation.args.cmd_arguments = args

    model_args = {
        'tokenizer': args.model_tokenizer,
        'default_system_message': args.model_default_system_message,
    }

    model_args = { k: v for k, v in model_args.items() if v is not None }

    if args.run_correctness_check:
        run_correctness_check(args.model_type, args.model_name, model_args)
        return

    if 'all' in args.benchmarks:
        args.benchmarks = all_benchmarks

    if os.path.exists('reports/__index__.json'):
        with open('reports/__index__.json') as f:
            models_and_benchmarks = merge_models_and_benchmarks_to_evaluate(json.load(f), args.model_type, args.model_name, args.benchmarks, model_args)

    evaluation_functions = [
        ('mt-bench', benchmarks.mt_bench.evaluate_model),
        ('human-eval-plus', benchmarks.human_eval_plus.evaluate_model),
        ('cot', benchmarks.cot.evaluate_model),
    ]

    with changed_exit_handlers():
        for item in models_and_benchmarks:
            for benchmark_name, evaluation_function in evaluation_functions:
                if benchmark_name in item['benchmarks']:
                    evaluation_function(item['model_type'], item['model_name'], item['model_args'], item['id'])
            unload_model()

    for item in models_and_benchmarks:
        if 'lm-evaluation-harness' in item['benchmarks']:
            benchmarks.lm_evaluation_harness.evaluate_model(item['model_type'], item['model_name'], item['model_args'], item['id'])

    join_threads()

    with open(os.path.join('reports', '__index__.json'), 'w') as f:
        f.write(json.dumps(models_and_benchmarks, indent=4) + '\n')

if __name__ == '__main__':
    main()
