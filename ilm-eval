#!/usr/bin/env python3

import multiprocessing

if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn')

import os
import json
import argparse
import threading
import uuid

import evaluation.args
from evaluation import benchmarks
from evaluation.utils import changed_exit_handlers
from evaluation.models.models import unload_model, create_model

def merge_models_and_benchmarks_to_evaluate(existing_models_and_benchmarks, new_model_type, new_model_name, new_benchmarks, model_args):
    if new_model_name is None:
        return existing_models_and_benchmarks

    model_args = { k: v for k, v in model_args.items() if v is not None }

    inserted_into_existing_entry = False
    for item in existing_models_and_benchmarks:
        if item['model_name'] != new_model_name:
            continue
        if item['model_type'] != new_model_type:
            raise # TODO: Only for now. Later we allow this.
        if item['model_args'] != model_args:
            raise # TODO: Only for now. Later we allow this.
        for benchmark in new_benchmarks:
            if benchmark not in item['benchmarks']:
                item['benchmarks'].insert(0, benchmark)
        inserted_into_existing_entry = True

    if inserted_into_existing_entry:
        return existing_models_and_benchmarks

    existing_models_and_benchmarks.insert(0, {
        'id': str(uuid.uuid4()),
        'model_type': new_model_type,
        'model_name': new_model_name,
        'benchmarks': new_benchmarks,
        'model_args': model_args,
    })

    return existing_models_and_benchmarks

def run_correctness_check(model_type, model_name, model_args):
    assert model_type is not None and model_name is not None

    def get_output():
        model = create_model(model_type, model_name, model_args)
        return model.reply([
            ('user', 'Please tell me a joke.'),
        ], temperature=0, max_new_tokens=50)

    previous_force_backend = evaluation.args.cmd_arguments.force_backend
    evaluation.args.cmd_arguments.force_backend = 'hf_transformers'

    hf_transformers_model_output = get_output()

    evaluation.args.cmd_arguments.force_backend = previous_force_backend

    default_backend_model_output = get_output()

    if hf_transformers_model_output != default_backend_model_output:
        raise Exception('Correctness check failed.\n'
            + '        DEFAULT BACKEND: ' + default_backend_model_output + '\n'
            + 'HF TRANSFORMERS BACKEND: ' + hf_transformers_model_output)

    unload_model()
    join_threads()

def join_threads():
    for thread in threading.enumerate():
        if thread.daemon:
            continue

        try:
            thread.join()
        except RuntimeError as error:
            if 'cannot join current thread' in error.args[0]: # main thread
                pass
            else:
                raise

def main():
    all_benchmarks = ['mt-bench', 'cot', 'human-eval-plus', 'lm-evaluation-harness']

    parser = argparse.ArgumentParser()
    parser.add_argument('-b', '--benchmarks', choices=['all'] + all_benchmarks, nargs='*', default='all',
        help='Benchmark(s) that the model will be evaluated on')
    parser.add_argument('-t', '--model-type',
        help='Type of the model that will be evaluated. Can be an API client name (openai), a prompt template (e.g. alpaca) or `fastchat` for the fastchat backend.')
    parser.add_argument('-m', '--model-name',
        help='Name of the model that will be evaluated. Depending on the type, it can be an OpenAI model name or a path to a huggingface model.')
    parser.add_argument('--model-tokenizer',
        help='By default, the tokenizer will be the same as the model, but it can also be overwritten with this argument.')
    parser.add_argument('--model-default-system-message',
        help='The default system message of the model. Only applicable for models that use a system message and only if no other system message has been specified.')
    parser.add_argument('--force-backend', choices=['vllm', 'tgi', 'hf_transformers'], required=False,
        help='Force a specific backend for model inference. By default, the backend will be selected automatically depending on model support, '
            + 'but if you encounter bugs with this you can overwrite the backend with this argument.')
    parser.add_argument('--tgi-max-batch-total-tokens', type=int, default=None)
    parser.add_argument('--run-correctness-check', action='store_true')
    args = parser.parse_args()

    evaluation.args.cmd_arguments = args

    model_args = {
        'tokenizer': args.model_tokenizer,
        'default_system_message': args.model_default_system_message,
    }

    if args.run_correctness_check:
        run_correctness_check(args.model_type, args.model_name, model_args)
        return

    if 'all' in args.benchmarks:
        args.benchmarks = all_benchmarks

    if os.path.exists('reports/__index__.json'):
        with open('reports/__index__.json') as f:
            models_and_benchmarks = merge_models_and_benchmarks_to_evaluate(json.load(f), args.model_type, args.model_name, args.benchmarks, model_args)

    evaluation_functions = [
        ('mt-bench', benchmarks.mt_bench.evaluate_model),
        ('human-eval-plus', benchmarks.human_eval_plus.evaluate_model),
        ('cot', benchmarks.cot.evaluate_model),
    ]

    with changed_exit_handlers():
        for item in models_and_benchmarks:
            for benchmark_name, evaluation_function in evaluation_functions:
                if benchmark_name in item['benchmarks']:
                    evaluation_function(item['model_type'], item['model_name'], item['model_args'], item['id'])
            unload_model()

    for item in models_and_benchmarks:
        if 'lm-evaluation-harness' in item['benchmarks']:
            benchmarks.lm_evaluation_harness.evaluate_model(item['model_type'], item['model_name'], item['model_args'], item['id'])

    join_threads()

    with open(os.path.join('reports', '__index__.json'), 'w') as f:
        f.write(json.dumps(models_and_benchmarks, indent=4) + '\n')

if __name__ == '__main__':
    main()
