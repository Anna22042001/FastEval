{
    "results": {
        "openbookqa": {
            "acc": 0.338,
            "acc_stderr": 0.021175665695209407,
            "acc_norm": 0.442,
            "acc_norm_stderr": 0.02223197069632112
        },
        "arc_easy": {
            "acc": 0.7584175084175084,
            "acc_stderr": 0.008783247004042162,
            "acc_norm": 0.632996632996633,
            "acc_norm_stderr": 0.009890173658452125
        },
        "winogrande": {
            "acc": 0.7079715864246251,
            "acc_stderr": 0.012779198491754015
        },
        "hellaswag": {
            "acc": 0.6055566620195181,
            "acc_stderr": 0.004877319683639072,
            "acc_norm": 0.7781318462457678,
            "acc_norm_stderr": 0.004146537488135699
        },
        "arc_challenge": {
            "acc": 0.45819112627986347,
            "acc_stderr": 0.014560220308714697,
            "acc_norm": 0.45819112627986347,
            "acc_norm_stderr": 0.0145602203087147
        },
        "piqa": {
            "acc": 0.7932535364526659,
            "acc_stderr": 0.009448665514183267,
            "acc_norm": 0.7812840043525572,
            "acc_norm_stderr": 0.009644731932667567
        },
        "boolq": {
            "acc": 0.7602446483180428,
            "acc_stderr": 0.007467124295862895
        }
    },
    "versions": {
        "openbookqa": 0,
        "arc_easy": 0,
        "winogrande": 0,
        "hellaswag": 0,
        "arc_challenge": 0,
        "piqa": 0,
        "boolq": 1
    },
    "config": {
        "model": "hf-causal-experimental",
        "model_args": "pretrained=NousResearch/Nous-Hermes-13b,dtype=float16,trust_remote_code=True,use_accelerate=True",
        "num_fewshot": 0,
        "batch_size": null,
        "device": null,
        "no_cache": false,
        "limit": null,
        "bootstrap_iters": 100000,
        "description_dict": null
    }
}