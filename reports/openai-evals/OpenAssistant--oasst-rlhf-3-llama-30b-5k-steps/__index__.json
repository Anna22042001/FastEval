{
    "aba_mrpc_true_false.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "aba_mrpc_true_false.dev.v0",
            "base_eval": "aba_mrpc_true_false",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "aba_mrpc_true_false/samples.jsonl"
                    },
                    "key": "aba_mrpc_true_false.dev.v0",
                    "group": "aba-mrpc-true-false"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618172054RYW3R5UD",
            "created_at": "2023-06-18 17:20:54.413402"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "algebra-word-problems.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "algebra-word-problems.s1.simple-v0",
            "base_eval": "algebra-word-problems",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "algebra_word_problems/samples.jsonl"
                    },
                    "key": "algebra-word-problems.s1.simple-v0",
                    "group": "algebra-word-problems"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618172552V5VKGUF6",
            "created_at": "2023-06-18 17:25:52.808360"
        },
        "final_report": {
            "accuracy": 0.16666666666666666
        }
    },
    "anagrams.test.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "anagrams.test.v1",
            "base_eval": "anagrams",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "few_shot_jsonl": "anagrams/fewshot.jsonl",
                        "num_few_shot": 5,
                        "samples_jsonl": "anagrams/samples.jsonl"
                    },
                    "key": "anagrams.test.v1",
                    "group": "anagrams"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181727226F4DS56E",
            "created_at": "2023-06-18 17:27:22.751731"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "balance-chemical-equation.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "balance-chemical-equation.dev.v0",
            "base_eval": "balance-chemical-equation",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "balance_chemical_equation/samples.jsonl"
                    },
                    "key": "balance-chemical-equation.dev.v0",
                    "group": "balance-chemical-equation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618173007FHZ7RZLX",
            "created_at": "2023-06-18 17:30:07.571442"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "match_banking77.test.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "match_banking77.test.v1",
            "base_eval": "match_banking77",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "banking77/samples.jsonl"
                    },
                    "key": "match_banking77.test.v1",
                    "group": "banking77"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618173412PRVAF7XN",
            "created_at": "2023-06-18 17:34:12.652534"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "belarusian-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "belarusian-lexicon.dev.v0",
            "base_eval": "belarusian-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "belarusian_lexicon/samples.jsonl"
                    },
                    "key": "belarusian-lexicon.dev.v0",
                    "group": "belarusian-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23061817355726JHWYCI",
            "created_at": "2023-06-18 17:35:57.273554"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "bigrams.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "bigrams.dev.v0",
            "base_eval": "bigrams",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "bigrams/samples.jsonl"
                    },
                    "key": "bigrams.dev.v0",
                    "group": "bigrams"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618173754EEVESWBV",
            "created_at": "2023-06-18 17:37:54.504601"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "bitwise.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "bitwise.dev.v0",
            "base_eval": "bitwise",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "bitwise/samples.jsonl"
                    },
                    "key": "bitwise.dev.v0",
                    "group": "bitwise"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618173851W5IGAI5N",
            "created_at": "2023-06-18 17:38:51.846265"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "born-first.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "born-first.dev.v0",
            "base_eval": "born-first",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "born_first/born_first.jsonl"
                    },
                    "key": "born-first.dev.v0",
                    "group": "born-first"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618174258OVYCEVMM",
            "created_at": "2023-06-18 17:42:58.966706"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "brazilian-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "brazilian-lexicon.dev.v0",
            "base_eval": "brazilian-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "brazilian-lexicon/samples.jsonl"
                    },
                    "key": "brazilian-lexicon.dev.v0",
                    "group": "brazilian-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618174720WJAB4IJS",
            "created_at": "2023-06-18 17:47:20.239895"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "bulgarian-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "bulgarian-lexicon.dev.v0",
            "base_eval": "bulgarian-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "bulgarian-lexicon/samples.jsonl"
                    },
                    "key": "bulgarian-lexicon.dev.v0",
                    "group": "bulgarian-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618174915YTEK4UDM",
            "created_at": "2023-06-18 17:49:15.544549"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "chess-piece-count.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "chess-piece-count.s1.simple-v0",
            "base_eval": "chess-piece-count",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "chess_piece_count/fuzzy_match.jsonl"
                    },
                    "key": "chess-piece-count.s1.simple-v0",
                    "group": "chess-piece-count"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618174931B6EFCPTA",
            "created_at": "2023-06-18 17:49:31.630533"
        },
        "final_report": {
            "accuracy": 0.0,
            "f1_score": 0.0
        }
    },
    "chess.match.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "chess.match.dev.v0",
            "base_eval": "chess",
            "split": "match",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "chess/match.jsonl"
                    },
                    "key": "chess.match.dev.v0",
                    "group": "chess"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618175256F3BJUS2U",
            "created_at": "2023-06-18 17:52:56.931694"
        },
        "final_report": {
            "accuracy": 0.3,
            "f1_score": 0.018553459119496858
        }
    },
    "compare-countries-area.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "compare-countries-area.dev.v0",
            "base_eval": "compare-countries-area",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "compare-countries-area/samples.jsonl"
                    },
                    "key": "compare-countries-area.dev.v0",
                    "group": "compare-countries-area"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181756062XRGKIVY",
            "created_at": "2023-06-18 17:56:06.294938"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "connect4.s1.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "connect4.s1.v1",
            "base_eval": "connect4",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "connect4/samples.jsonl"
                    },
                    "key": "connect4.s1.v1",
                    "group": "connect-4"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618175828OSDOTXY7",
            "created_at": "2023-06-18 17:58:28.167884"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "convert-hex-hsl-lightness.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "convert-hex-hsl-lightness.dev.v0",
            "base_eval": "convert-hex-hsl-lightness",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "convert-hex-hsl-lightness/samples.jsonl"
                    },
                    "key": "convert-hex-hsl-lightness.dev.v0",
                    "group": "convert-hex-hsl-lightness"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618180431LN7NEW5Y",
            "created_at": "2023-06-18 18:04:31.597283"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "coqa-match.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "coqa-match.dev.v0",
            "base_eval": "coqa-match",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "coqa/match.jsonl"
                    },
                    "key": "coqa-match.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618180511QGAOQSMF",
            "created_at": "2023-06-18 18:05:11.855072"
        },
        "final_report": {
            "accuracy": 0.6
        }
    },
    "coqa-closedqa-correct.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                "openai:gpt-3.5-turbo"
            ],
            "eval_name": "coqa-closedqa-correct.dev.v0",
            "base_eval": "coqa-closedqa-correct",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                    "openai:gpt-3.5-turbo"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "modelgraded_spec": "closedqa",
                        "modelgraded_spec_args": {
                            "criteria": "correctness: Is the answer correct?"
                        }
                    },
                    "key": "coqa-closedqa-correct.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618180612YBZ6KHBR",
            "created_at": "2023-06-18 18:06:12.528846"
        },
        "final_report": {
            "counts/Y": 5,
            "counts/N": 4,
            "score": 0.5555555555555556
        }
    },
    "coqa-closedqa-relevance.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                "openai:gpt-3.5-turbo"
            ],
            "eval_name": "coqa-closedqa-relevance.dev.v0",
            "base_eval": "coqa-closedqa-relevance",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                    "openai:gpt-3.5-turbo"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "modelgraded_spec": "closedqa",
                        "modelgraded_spec_args": {
                            "criteria": "relevance: Is the submission referring to a real quote from the text?"
                        }
                    },
                    "key": "coqa-closedqa-relevance.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618180912HGMVMQHD",
            "created_at": "2023-06-18 18:09:12.179094"
        },
        "final_report": {
            "counts/Y": 7,
            "counts/N": 2,
            "score": 0.7777777777777778
        }
    },
    "coqa-closedqa-conciseness.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                "openai:gpt-3.5-turbo"
            ],
            "eval_name": "coqa-closedqa-conciseness.dev.v0",
            "base_eval": "coqa-closedqa-conciseness",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                    "openai:gpt-3.5-turbo"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "modelgraded_spec": "closedqa",
                        "modelgraded_spec_args": {
                            "criteria": "conciseness: Is the answer concise and to the point?"
                        }
                    },
                    "key": "coqa-closedqa-conciseness.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618181115XM6AI2OG",
            "created_at": "2023-06-18 18:11:15.841003"
        },
        "final_report": {
            "counts/Y": 4,
            "counts/N": 5,
            "score": 0.4444444444444444
        }
    },
    "crepe.dev.v2.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "crepe.dev.v2",
            "base_eval": "crepe",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "crepe/samples.jsonl"
                    },
                    "key": "crepe.dev.v2",
                    "group": "crepe"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618181227DFAPXX4H",
            "created_at": "2023-06-18 18:12:27.559134"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "cube-pack.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "cube-pack.dev.v0",
            "base_eval": "cube-pack",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "cube-pack/samples.jsonl"
                    },
                    "key": "cube-pack.dev.v0",
                    "group": "cube-pack"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181816265IYVOTVY",
            "created_at": "2023-06-18 18:16:26.763935"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "decrypt-caesar-cipher.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "decrypt-caesar-cipher.dev.v0",
            "base_eval": "decrypt-caesar-cipher",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "decrypt_caesar_cipher/samples.jsonl"
                    },
                    "key": "decrypt-caesar-cipher.dev.v0",
                    "group": "decrypt-caesar-cipher"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618181714ZLBCSNHU",
            "created_at": "2023-06-18 18:17:14.653065"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "determinant.test.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "determinant.test.v1",
            "base_eval": "determinant",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "determinant/samples.jsonl"
                    },
                    "key": "determinant.test.v1",
                    "group": "determinant"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181819533JQKVJ5B",
            "created_at": "2023-06-18 18:19:53.143483"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "diagrammatic_logic.dev.v2.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "diagrammatic_logic.dev.v2",
            "base_eval": "diagrammatic_logic",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "diagrammatic_logic/samples.jsonl"
                    },
                    "key": "diagrammatic_logic.dev.v2",
                    "group": "diagrammatic_logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618182156IN5TC6SS",
            "created_at": "2023-06-18 18:21:56.677054"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "dice-rotation-sequence.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "dice-rotation-sequence.dev.v0",
            "base_eval": "dice-rotation-sequence",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "dice-rotation-sequence/samples.jsonl"
                    },
                    "key": "dice-rotation-sequence.dev.v0",
                    "group": "dice-rotation-sequence"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618182416354HD3YO",
            "created_at": "2023-06-18 18:24:16.723475"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "dutch-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "dutch-lexicon.dev.v0",
            "base_eval": "dutch-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "dutch-lexicon/samples.jsonl"
                    },
                    "key": "dutch-lexicon.dev.v0",
                    "group": "dutch-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618182445LXO5QGGW",
            "created_at": "2023-06-18 18:24:45.204423"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "emoji-riddle.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "emoji-riddle.s1.simple-v0",
            "base_eval": "emoji-riddle",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "emoji_riddle/fuzzy_match.jsonl"
                    },
                    "key": "emoji-riddle.s1.simple-v0",
                    "group": "emoji-riddle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618182550USNL2JFT",
            "created_at": "2023-06-18 18:25:50.841622"
        },
        "final_report": {
            "accuracy": 0.1,
            "f1_score": 0.16374178403755868
        }
    },
    "emotional-intelligence.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "emotional-intelligence.dev.v0",
            "base_eval": "emotional-intelligence",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "emotional-intelligence/samples.jsonl"
                    },
                    "key": "emotional-intelligence.dev.v0",
                    "group": "emotional-intelligence"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181827276VA3WQZJ",
            "created_at": "2023-06-18 18:27:27.607848"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "escher-sentences.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "escher-sentences.dev.v0",
            "base_eval": "escher-sentences",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "escher_sentences/samples.jsonl"
                    },
                    "key": "escher-sentences.dev.v0",
                    "group": "escher-sentences"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181830477NUNQHXW",
            "created_at": "2023-06-18 18:30:47.659089"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "fcc_amateur_extra.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "fcc_amateur_extra.dev.v0",
            "base_eval": "fcc_amateur_extra",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "fcc_amateur_extra/samples.jsonl"
                    },
                    "key": "fcc_amateur_extra.dev.v0",
                    "group": "fcc_amateur_extra"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618183509KOJLSWIL",
            "created_at": "2023-06-18 18:35:09.795909"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "first-letters.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "first-letters.dev.v0",
            "base_eval": "first-letters",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "first-letters/samples.jsonl"
                    },
                    "key": "first-letters.dev.v0",
                    "group": "first-letters"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618183952RWEWEY36",
            "created_at": "2023-06-18 18:39:52.255057"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "formal-logic.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "formal-logic.dev.v0",
            "base_eval": "formal-logic",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "formal_logic/formal_logic_expressions.jsonl"
                    },
                    "key": "formal-logic.dev.v0",
                    "group": "formal_logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23061818410944K5BO7I",
            "created_at": "2023-06-18 18:41:09.092839"
        },
        "final_report": {
            "accuracy": 0.6
        }
    },
    "forth-stack-sim.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "forth-stack-sim.dev.v0",
            "base_eval": "forth-stack-sim",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "forth_stack_sim/samples.jsonl"
                    },
                    "key": "forth-stack-sim.dev.v0",
                    "group": "forth-stack-sim"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618184134MVPAY4GR",
            "created_at": "2023-06-18 18:41:34.784166"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "forth-stack-sim-basic.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "forth-stack-sim-basic.dev.v0",
            "base_eval": "forth-stack-sim-basic",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "forth_stack_sim/basic_samples.jsonl"
                    },
                    "key": "forth-stack-sim-basic.dev.v0",
                    "group": "forth-stack-sim"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618184231HMYJZKKL",
            "created_at": "2023-06-18 18:42:31.306648"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "forth-stack-sim-detailed.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "forth-stack-sim-detailed.dev.v0",
            "base_eval": "forth-stack-sim-detailed",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "forth_stack_sim/detailed_samples.jsonl"
                    },
                    "key": "forth-stack-sim-detailed.dev.v0",
                    "group": "forth-stack-sim"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618184309ALXYHXEO",
            "created_at": "2023-06-18 18:43:09.114229"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "greek-vocabulary.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "greek-vocabulary.dev.v0",
            "base_eval": "greek-vocabulary",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "greek_vocabulary/samples.jsonl"
                    },
                    "key": "greek-vocabulary.dev.v0",
                    "group": "greek-vocabulary"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181846032NRDECMO",
            "created_at": "2023-06-18 18:46:03.422101"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "heart-disease.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "heart-disease.v0",
            "base_eval": "heart-disease",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "heart-disease/samples.jsonl"
                    },
                    "key": "heart-disease.v0",
                    "group": "heart-disease"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181847407ARZSM6A",
            "created_at": "2023-06-18 18:47:40.473685"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "hebrew-rhyme.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "hebrew-rhyme.v0",
            "base_eval": "hebrew-rhyme",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "hebrew_rhyme/samples.jsonl"
                    },
                    "key": "hebrew-rhyme.v0",
                    "group": "hebrew-rhyme"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181848293JJCAUWA",
            "created_at": "2023-06-18 18:48:29.418762"
        },
        "final_report": {
            "accuracy": 0.2,
            "f1_score": 0.1590909090909091
        }
    },
    "hindi_upsc.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "hindi_upsc.dev.v0",
            "base_eval": "hindi_upsc",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "hindi_upsc/samples.jsonl"
                    },
                    "key": "hindi_upsc.dev.v0",
                    "group": "hindi_upsc"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618185035WY2FWSVG",
            "created_at": "2023-06-18 18:50:35.506804"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "imperial_date_to_string.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "imperial_date_to_string.dev.v0",
            "base_eval": "imperial_date_to_string",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "imperial_date_to_string/samples.jsonl"
                    },
                    "key": "imperial_date_to_string.dev.v0",
                    "group": "imperial_date_to_string"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618185619GFUQCSIS",
            "created_at": "2023-06-18 18:56:19.411942"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "infiniteloop-match.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "infiniteloop-match.s1.simple-v0",
            "base_eval": "infiniteloop-match",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "infiniteloop-match/infiniteloop-match.jsonl"
                    },
                    "key": "infiniteloop-match.s1.simple-v0",
                    "group": "infiniteloop-match"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618185740QESDOJJJ",
            "created_at": "2023-06-18 18:57:40.297996"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "invoices.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "invoices.dev.v0",
            "base_eval": "invoices",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "invoices/match.jsonl"
                    },
                    "key": "invoices.dev.v0",
                    "group": "invoices"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618190000T7VVTQD5",
            "created_at": "2023-06-18 19:00:00.342255"
        },
        "final_report": {
            "accuracy": 0.7
        }
    },
    "japanese-national-medical-exam01.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "japanese-national-medical-exam01.dev.v0",
            "base_eval": "japanese-national-medical-exam01",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "japanese-national-medical-exam01/japanese-national-medical-exam01.jsonl"
                    },
                    "key": "japanese-national-medical-exam01.dev.v0",
                    "group": "japanese-national-medical-exam01"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618190203QDQLOL6C",
            "created_at": "2023-06-18 19:02:03.194876"
        },
        "final_report": {
            "accuracy": 0.26666666666666666
        }
    },
    "japanese_driving_license.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "japanese_driving_license.s1.simple-v0",
            "base_eval": "japanese_driving_license",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "japanese_driving_license/samples.jsonl"
                    },
                    "key": "japanese_driving_license.s1.simple-v0",
                    "group": "japanese_driving_license"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618190555AFV56GCL",
            "created_at": "2023-06-18 19:05:55.623739"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "job_listing_title_for_a_caregiver_in_japan.test.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "job_listing_title_for_a_caregiver_in_japan.test.v1",
            "base_eval": "job_listing_title_for_a_caregiver_in_japan",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "job_listing_title_for_a_caregiver_in_japan/samples.jsonl"
                    },
                    "key": "job_listing_title_for_a_caregiver_in_japan.test.v1",
                    "group": "job_listing_title_for_a_caregiver_in_japan"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618190846VNC44LTW",
            "created_at": "2023-06-18 19:08:46.409599"
        },
        "final_report": {
            "accuracy": 0.5333333333333333
        }
    },
    "knot-theory-unknotting-problem.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "knot-theory-unknotting-problem.dev.v0",
            "base_eval": "knot-theory-unknotting-problem",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "knot-theory/knot-theory-unknotting-problems.jsonl"
                    },
                    "key": "knot-theory-unknotting-problem.dev.v0",
                    "group": "knot-theory"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618191333MVSTO4QG",
            "created_at": "2023-06-18 19:13:33.277320"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "knot-theory-unknotting-number.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "knot-theory-unknotting-number.dev.v0",
            "base_eval": "knot-theory-unknotting-number",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "knot-theory/knot-theory-unknotting-numbers.jsonl"
                    },
                    "key": "knot-theory-unknotting-number.dev.v0",
                    "group": "knot-theory"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181913352UHENZWT",
            "created_at": "2023-06-18 19:13:35.962946"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "hellaswag.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "hellaswag.val.ab-v1",
            "base_eval": "hellaswag",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://hellaswag?split=validation",
                        "instructions": "Choose the most plausible continuation for the story."
                    },
                    "key": "hellaswag.val.ab-v1",
                    "group": "language"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618191703Z2GNUEFO",
            "created_at": "2023-06-18 19:17:03.000914"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "last-word-nth.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "last-word-nth.s1.simple-v0",
            "base_eval": "last-word-nth",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "last_word_nth/samples.jsonl"
                    },
                    "key": "last-word-nth.s1.simple-v0",
                    "group": "last-word-nth"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618191951KORUU7IH",
            "created_at": "2023-06-18 19:19:51.792568"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "lat_long_identify.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "lat_long_identify.dev.v0",
            "base_eval": "lat_long_identify",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "lat_long_identify/samples.jsonl"
                    },
                    "key": "lat_long_identify.dev.v0",
                    "group": "lat_long_identify"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181922162V3BBGCR",
            "created_at": "2023-06-18 19:22:16.720357"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "logic-statements.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "logic-statements.dev.v0",
            "base_eval": "logic-statements",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "logic-statements/logic-statements.jsonl"
                    },
                    "key": "logic-statements.dev.v0",
                    "group": "logic-statements"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618193027RESDDY5H",
            "created_at": "2023-06-18 19:30:27.374724"
        },
        "final_report": {
            "accuracy": 0.55,
            "f1_score": 0.3606060606060606
        }
    },
    "logiqa.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "logiqa.dev.v0",
            "base_eval": "logiqa",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "logiqa/logiqa.jsonl"
                    },
                    "key": "logiqa.dev.v0",
                    "group": "logiqa"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181935527T7RTAD6",
            "created_at": "2023-06-18 19:35:52.274044"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "map-electronic-component-part-to-fact.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "map-electronic-component-part-to-fact.dev.v0",
            "base_eval": "map-electronic-component-part-to-fact",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "map-electronic-component-part-to-fact/samples.jsonl"
                    },
                    "key": "map-electronic-component-part-to-fact.dev.v0",
                    "group": "map-electronic-component-part-to-fact"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618194020KLBCE644",
            "created_at": "2023-06-18 19:40:20.421874"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "medmcqa.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "medmcqa.dev.v0",
            "base_eval": "medmcqa",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "medmcqa/samples.jsonl"
                    },
                    "key": "medmcqa.dev.v0",
                    "group": "medmcqa"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618194229BDSRMST3",
            "created_at": "2023-06-18 19:42:29.373974"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "mendelian_inheritance.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mendelian_inheritance.dev.v0",
            "base_eval": "mendelian_inheritance",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "mendelian_inheritance/samples.jsonl"
                    },
                    "key": "mendelian_inheritance.dev.v0",
                    "group": "mendelian_inheritance"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618194324KYY75JBS",
            "created_at": "2023-06-18 19:43:24.256383"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-abstract-algebra.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-abstract-algebra.val.ab-v1",
            "base_eval": "mmlu-abstract-algebra",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=abstract_algebra&split=validation"
                    },
                    "key": "mmlu-abstract-algebra.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181947392W6JNCPJ",
            "created_at": "2023-06-18 19:47:39.242002"
        },
        "final_report": {
            "accuracy": 0.45454545454545453
        }
    },
    "mmlu-anatomy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-anatomy.val.ab-v1",
            "base_eval": "mmlu-anatomy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=anatomy&split=validation"
                    },
                    "key": "mmlu-anatomy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306181949532RGE2VTS",
            "created_at": "2023-06-18 19:49:53.085171"
        },
        "final_report": {
            "accuracy": 0.42857142857142855
        }
    },
    "mmlu-astronomy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-astronomy.val.ab-v1",
            "base_eval": "mmlu-astronomy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=astronomy&split=validation"
                    },
                    "key": "mmlu-astronomy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618195031PJJPKDCI",
            "created_at": "2023-06-18 19:50:31.426540"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "mmlu-business-ethics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-business-ethics.val.ab-v1",
            "base_eval": "mmlu-business-ethics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=business_ethics&split=validation"
                    },
                    "key": "mmlu-business-ethics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618195322CW22OF7U",
            "created_at": "2023-06-18 19:53:22.914529"
        },
        "final_report": {
            "accuracy": 0.36363636363636365
        }
    },
    "mmlu-clinical-knowledge.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-clinical-knowledge.val.ab-v1",
            "base_eval": "mmlu-clinical-knowledge",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=clinical_knowledge&split=validation"
                    },
                    "key": "mmlu-clinical-knowledge.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618195503QRREWY22",
            "created_at": "2023-06-18 19:55:03.880215"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "mmlu-college-biology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-biology.val.ab-v1",
            "base_eval": "mmlu-college-biology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_biology&split=validation"
                    },
                    "key": "mmlu-college-biology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618195712DIEOM5XM",
            "created_at": "2023-06-18 19:57:12.675057"
        },
        "final_report": {
            "accuracy": 0.1875
        }
    },
    "mmlu-college-chemistry.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-chemistry.val.ab-v1",
            "base_eval": "mmlu-college-chemistry",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_chemistry&split=validation"
                    },
                    "key": "mmlu-college-chemistry.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618195925ZQERFXS4",
            "created_at": "2023-06-18 19:59:25.058898"
        },
        "final_report": {
            "accuracy": 0.375
        }
    },
    "mmlu-college-computer-science.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-computer-science.val.ab-v1",
            "base_eval": "mmlu-college-computer-science",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_computer_science&split=validation"
                    },
                    "key": "mmlu-college-computer-science.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618200109ONIPHKD3",
            "created_at": "2023-06-18 20:01:09.177452"
        },
        "final_report": {
            "accuracy": 0.2727272727272727
        }
    },
    "mmlu-college-mathematics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-mathematics.val.ab-v1",
            "base_eval": "mmlu-college-mathematics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_mathematics&split=validation"
                    },
                    "key": "mmlu-college-mathematics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182004004N2MWMG7",
            "created_at": "2023-06-18 20:04:00.926851"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-college-medicine.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-medicine.val.ab-v1",
            "base_eval": "mmlu-college-medicine",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_medicine&split=validation"
                    },
                    "key": "mmlu-college-medicine.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618200615OZOXFVHJ",
            "created_at": "2023-06-18 20:06:15.848805"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "mmlu-college-physics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-physics.val.ab-v1",
            "base_eval": "mmlu-college-physics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_physics&split=validation"
                    },
                    "key": "mmlu-college-physics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618200848DLF366IG",
            "created_at": "2023-06-18 20:08:48.478648"
        },
        "final_report": {
            "accuracy": 0.36363636363636365
        }
    },
    "mmlu-computer-security.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-computer-security.val.ab-v1",
            "base_eval": "mmlu-computer-security",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=computer_security&split=validation"
                    },
                    "key": "mmlu-computer-security.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182011365RYQRHQ6",
            "created_at": "2023-06-18 20:11:36.328186"
        },
        "final_report": {
            "accuracy": 0.36363636363636365
        }
    },
    "mmlu-conceptual-physics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-conceptual-physics.val.ab-v1",
            "base_eval": "mmlu-conceptual-physics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=conceptual_physics&split=validation"
                    },
                    "key": "mmlu-conceptual-physics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618201415UPL75BTJ",
            "created_at": "2023-06-18 20:14:15.818031"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "mmlu-econometrics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-econometrics.val.ab-v1",
            "base_eval": "mmlu-econometrics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=econometrics&split=validation"
                    },
                    "key": "mmlu-econometrics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618201613U75L7HVD",
            "created_at": "2023-06-18 20:16:13.569634"
        },
        "final_report": {
            "accuracy": 0.16666666666666666
        }
    },
    "mmlu-electrical-engineering.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-electrical-engineering.val.ab-v1",
            "base_eval": "mmlu-electrical-engineering",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=electrical_engineering&split=validation"
                    },
                    "key": "mmlu-electrical-engineering.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182018053ZK5KT5W",
            "created_at": "2023-06-18 20:18:05.427796"
        },
        "final_report": {
            "accuracy": 0.375
        }
    },
    "mmlu-elementary-mathematics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-elementary-mathematics.val.ab-v1",
            "base_eval": "mmlu-elementary-mathematics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=elementary_mathematics&split=validation"
                    },
                    "key": "mmlu-elementary-mathematics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618201928VNOZXDZH",
            "created_at": "2023-06-18 20:19:28.763659"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "mmlu-formal-logic.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-formal-logic.val.ab-v1",
            "base_eval": "mmlu-formal-logic",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=formal_logic&split=validation"
                    },
                    "key": "mmlu-formal-logic.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618202233EX66JB6H",
            "created_at": "2023-06-18 20:22:33.046473"
        },
        "final_report": {
            "accuracy": 0.21428571428571427
        }
    },
    "mmlu-global-facts.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-global-facts.val.ab-v1",
            "base_eval": "mmlu-global-facts",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=global_facts&split=validation"
                    },
                    "key": "mmlu-global-facts.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618202456A6KUJZUH",
            "created_at": "2023-06-18 20:24:56.947406"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "mmlu-high-school-biology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-biology.val.ab-v1",
            "base_eval": "mmlu-high-school-biology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_biology&split=validation"
                    },
                    "key": "mmlu-high-school-biology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618202551EU73J4HQ",
            "created_at": "2023-06-18 20:25:51.187065"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "mmlu-high-school-chemistry.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-chemistry.val.ab-v1",
            "base_eval": "mmlu-high-school-chemistry",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_chemistry&split=validation"
                    },
                    "key": "mmlu-high-school-chemistry.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618202941QVGKTPV4",
            "created_at": "2023-06-18 20:29:41.213149"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "mmlu-high-school-computer-science.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-computer-science.val.ab-v1",
            "base_eval": "mmlu-high-school-computer-science",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_computer_science&split=validation"
                    },
                    "key": "mmlu-high-school-computer-science.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618203310VRWWEEIY",
            "created_at": "2023-06-18 20:33:10.485734"
        },
        "final_report": {
            "accuracy": 0.2222222222222222
        }
    },
    "mmlu-high-school-european-history.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-european-history.val.ab-v1",
            "base_eval": "mmlu-high-school-european-history",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_european_history&split=validation"
                    },
                    "key": "mmlu-high-school-european-history.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618203425RS4X2GPF",
            "created_at": "2023-06-18 20:34:25.196329"
        },
        "final_report": {
            "accuracy": 0.3888888888888889
        }
    },
    "mmlu-high-school-geography.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-geography.val.ab-v1",
            "base_eval": "mmlu-high-school-geography",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_geography&split=validation"
                    },
                    "key": "mmlu-high-school-geography.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618203705OOAVV2CL",
            "created_at": "2023-06-18 20:37:05.573493"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "mmlu-high-school-government-and-politics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-government-and-politics.val.ab-v1",
            "base_eval": "mmlu-high-school-government-and-politics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_government_and_politics&split=validation"
                    },
                    "key": "mmlu-high-school-government-and-politics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618204033QMTJPHHV",
            "created_at": "2023-06-18 20:40:33.565020"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "mmlu-high-school-macroeconomics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-macroeconomics.val.ab-v1",
            "base_eval": "mmlu-high-school-macroeconomics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_macroeconomics&split=validation"
                    },
                    "key": "mmlu-high-school-macroeconomics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618204313Q54BXFKH",
            "created_at": "2023-06-18 20:43:13.321099"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "mmlu-high-school-mathematics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-mathematics.val.ab-v1",
            "base_eval": "mmlu-high-school-mathematics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_mathematics&split=validation"
                    },
                    "key": "mmlu-high-school-mathematics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618204608BGFCY6AP",
            "created_at": "2023-06-18 20:46:08.708354"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "mmlu-high-school-microeconomics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-microeconomics.val.ab-v1",
            "base_eval": "mmlu-high-school-microeconomics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_microeconomics&split=validation"
                    },
                    "key": "mmlu-high-school-microeconomics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618204952C2U7DP3G",
            "created_at": "2023-06-18 20:49:52.885176"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "mmlu-high-school-physics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-physics.val.ab-v1",
            "base_eval": "mmlu-high-school-physics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_physics&split=validation"
                    },
                    "key": "mmlu-high-school-physics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182052466TX5QHY7",
            "created_at": "2023-06-18 20:52:46.371174"
        },
        "final_report": {
            "accuracy": 0.29411764705882354
        }
    },
    "mmlu-high-school-psychology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-psychology.val.ab-v1",
            "base_eval": "mmlu-high-school-psychology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_psychology&split=validation"
                    },
                    "key": "mmlu-high-school-psychology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618205605ROEED4KN",
            "created_at": "2023-06-18 20:56:05.149138"
        },
        "final_report": {
            "accuracy": 0.6
        }
    },
    "mmlu-high-school-statistics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-statistics.val.ab-v1",
            "base_eval": "mmlu-high-school-statistics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_statistics&split=validation"
                    },
                    "key": "mmlu-high-school-statistics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618205843E2ESIGU4",
            "created_at": "2023-06-18 20:58:43.017286"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "mmlu-high-school-us-history.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-us-history.val.ab-v1",
            "base_eval": "mmlu-high-school-us-history",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_us_history&split=validation"
                    },
                    "key": "mmlu-high-school-us-history.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23061821030747J6TFJW",
            "created_at": "2023-06-18 21:03:07.770982"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "mmlu-high-school-world-history.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-world-history.val.ab-v1",
            "base_eval": "mmlu-high-school-world-history",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_world_history&split=validation"
                    },
                    "key": "mmlu-high-school-world-history.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618210645BGWPRN5T",
            "created_at": "2023-06-18 21:06:45.636749"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "mmlu-human-aging.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-human-aging.val.ab-v1",
            "base_eval": "mmlu-human-aging",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=human_aging&split=validation"
                    },
                    "key": "mmlu-human-aging.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618210927LH4ZZXFC",
            "created_at": "2023-06-18 21:09:27.942335"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "mmlu-human-sexuality.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-human-sexuality.val.ab-v1",
            "base_eval": "mmlu-human-sexuality",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=human_sexuality&split=validation"
                    },
                    "key": "mmlu-human-sexuality.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618211149NK2KIQH2",
            "created_at": "2023-06-18 21:11:49.930922"
        },
        "final_report": {
            "accuracy": 0.16666666666666666
        }
    },
    "mmlu-international-law.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-international-law.val.ab-v1",
            "base_eval": "mmlu-international-law",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=international_law&split=validation"
                    },
                    "key": "mmlu-international-law.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182114024EB4KMJY",
            "created_at": "2023-06-18 21:14:02.430931"
        },
        "final_report": {
            "accuracy": 0.38461538461538464
        }
    },
    "mmlu-jurisprudence.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-jurisprudence.val.ab-v1",
            "base_eval": "mmlu-jurisprudence",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=jurisprudence&split=validation"
                    },
                    "key": "mmlu-jurisprudence.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618211538KDIL3JU6",
            "created_at": "2023-06-18 21:15:38.328108"
        },
        "final_report": {
            "accuracy": 0.45454545454545453
        }
    },
    "mmlu-logical-fallacies.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-logical-fallacies.val.ab-v1",
            "base_eval": "mmlu-logical-fallacies",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=logical_fallacies&split=validation"
                    },
                    "key": "mmlu-logical-fallacies.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618211626EMNUEP7V",
            "created_at": "2023-06-18 21:16:26.488982"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "mmlu-machine-learning.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-machine-learning.val.ab-v1",
            "base_eval": "mmlu-machine-learning",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=machine_learning&split=validation"
                    },
                    "key": "mmlu-machine-learning.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618211855XONGFNFN",
            "created_at": "2023-06-18 21:18:55.475262"
        },
        "final_report": {
            "accuracy": 0.2727272727272727
        }
    },
    "mmlu-management.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-management.val.ab-v1",
            "base_eval": "mmlu-management",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=management&split=validation"
                    },
                    "key": "mmlu-management.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618212107EPCKHOZO",
            "created_at": "2023-06-18 21:21:07.823280"
        },
        "final_report": {
            "accuracy": 0.36363636363636365
        }
    },
    "mmlu-marketing.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-marketing.val.ab-v1",
            "base_eval": "mmlu-marketing",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=marketing&split=validation"
                    },
                    "key": "mmlu-marketing.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618212242C3DNEJPX",
            "created_at": "2023-06-18 21:22:42.036891"
        },
        "final_report": {
            "accuracy": 0.7
        }
    },
    "mmlu-medical-genetics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-medical-genetics.val.ab-v1",
            "base_eval": "mmlu-medical-genetics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=medical_genetics&split=validation"
                    },
                    "key": "mmlu-medical-genetics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618212408TO4SSAHC",
            "created_at": "2023-06-18 21:24:08.805910"
        },
        "final_report": {
            "accuracy": 0.8181818181818182
        }
    },
    "mmlu-miscellaneous.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-miscellaneous.val.ab-v1",
            "base_eval": "mmlu-miscellaneous",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=miscellaneous&split=validation"
                    },
                    "key": "mmlu-miscellaneous.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618212632UG3SYALH",
            "created_at": "2023-06-18 21:26:32.914544"
        },
        "final_report": {
            "accuracy": 0.65
        }
    },
    "mmlu-moral-disputes.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-moral-disputes.val.ab-v1",
            "base_eval": "mmlu-moral-disputes",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=moral_disputes&split=validation"
                    },
                    "key": "mmlu-moral-disputes.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618212820HBCYPJT3",
            "created_at": "2023-06-18 21:28:20.920195"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "mmlu-moral-scenarios.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-moral-scenarios.val.ab-v1",
            "base_eval": "mmlu-moral-scenarios",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=moral_scenarios&split=validation"
                    },
                    "key": "mmlu-moral-scenarios.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618213040AL2LJMJL",
            "created_at": "2023-06-18 21:30:40.219689"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "mmlu-nutrition.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-nutrition.val.ab-v1",
            "base_eval": "mmlu-nutrition",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=nutrition&split=validation"
                    },
                    "key": "mmlu-nutrition.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618213349AOG37JRZ",
            "created_at": "2023-06-18 21:33:49.726404"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "mmlu-philosophy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-philosophy.val.ab-v1",
            "base_eval": "mmlu-philosophy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=philosophy&split=validation"
                    },
                    "key": "mmlu-philosophy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618213756LD6QRIWP",
            "created_at": "2023-06-18 21:37:56.710049"
        },
        "final_report": {
            "accuracy": 0.6
        }
    },
    "mmlu-prehistory.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-prehistory.val.ab-v1",
            "base_eval": "mmlu-prehistory",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=prehistory&split=validation"
                    },
                    "key": "mmlu-prehistory.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182140232MDMO6FM",
            "created_at": "2023-06-18 21:40:23.455208"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "mmlu-professional-accounting.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-professional-accounting.val.ab-v1",
            "base_eval": "mmlu-professional-accounting",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_accounting&split=validation"
                    },
                    "key": "mmlu-professional-accounting.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618214304BCIYZXDQ",
            "created_at": "2023-06-18 21:43:04.424283"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "mmlu-professional-law.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-professional-law.val.ab-v1",
            "base_eval": "mmlu-professional-law",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_law&split=validation"
                    },
                    "key": "mmlu-professional-law.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618214552TGEIPRC4",
            "created_at": "2023-06-18 21:45:52.320354"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "mmlu-professional-medicine.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-professional-medicine.val.ab-v1",
            "base_eval": "mmlu-professional-medicine",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_medicine&split=validation"
                    },
                    "key": "mmlu-professional-medicine.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618214906MFMHMANZ",
            "created_at": "2023-06-18 21:49:06.366856"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "mmlu-professional-psychology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-professional-psychology.val.ab-v1",
            "base_eval": "mmlu-professional-psychology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_psychology&split=validation"
                    },
                    "key": "mmlu-professional-psychology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618215211OV2U46YZ",
            "created_at": "2023-06-18 21:52:11.840117"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "mmlu-public-relations.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-public-relations.val.ab-v1",
            "base_eval": "mmlu-public-relations",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=public_relations&split=validation"
                    },
                    "key": "mmlu-public-relations.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618215438WMVFDRU5",
            "created_at": "2023-06-18 21:54:38.293707"
        },
        "final_report": {
            "accuracy": 0.3333333333333333
        }
    },
    "mmlu-security-studies.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-security-studies.val.ab-v1",
            "base_eval": "mmlu-security-studies",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=security_studies&split=validation"
                    },
                    "key": "mmlu-security-studies.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618215623DXAQLY2I",
            "created_at": "2023-06-18 21:56:23.183142"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "mmlu-sociology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-sociology.val.ab-v1",
            "base_eval": "mmlu-sociology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=sociology&split=validation"
                    },
                    "key": "mmlu-sociology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618215933B2V6KYKS",
            "created_at": "2023-06-18 21:59:33.543239"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "mmlu-us-foreign-policy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-us-foreign-policy.val.ab-v1",
            "base_eval": "mmlu-us-foreign-policy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=us_foreign_policy&split=validation"
                    },
                    "key": "mmlu-us-foreign-policy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618220211RPNDMGGA",
            "created_at": "2023-06-18 22:02:11.492315"
        },
        "final_report": {
            "accuracy": 0.6363636363636364
        }
    },
    "mmlu-virology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-virology.val.ab-v1",
            "base_eval": "mmlu-virology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=virology&split=validation"
                    },
                    "key": "mmlu-virology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618220428OPBM5EKQ",
            "created_at": "2023-06-18 22:04:28.971311"
        },
        "final_report": {
            "accuracy": 0.2777777777777778
        }
    },
    "mmlu-world-religions.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-world-religions.val.ab-v1",
            "base_eval": "mmlu-world-religions",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=world_religions&split=validation"
                    },
                    "key": "mmlu-world-religions.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618220853QFDBW2IW",
            "created_at": "2023-06-18 22:08:53.314684"
        },
        "final_report": {
            "accuracy": 0.47368421052631576
        }
    },
    "moral_exceptQA.test.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "moral_exceptQA.test.v1",
            "base_eval": "moral_exceptQA",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "moral_exceptQA/samples.jsonl"
                    },
                    "key": "moral_exceptQA.test.v1",
                    "group": "moral_exceptQA"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618221159DCIEKSYW",
            "created_at": "2023-06-18 22:11:59.191204"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "multi-step-equations.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "multi-step-equations.dev.v0",
            "base_eval": "multi-step-equations",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "multi-step-equations/samples.jsonl"
                    },
                    "key": "multi-step-equations.dev.v0",
                    "group": "multi-step-equations"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618221646F7IIGUP4",
            "created_at": "2023-06-18 22:16:46.614835"
        },
        "final_report": {
            "accuracy": 0.26666666666666666
        }
    },
    "naughty_strings_fuzzy.test.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "naughty_strings_fuzzy.test.v1",
            "base_eval": "naughty_strings_fuzzy",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "naughty_strings/samples.jsonl"
                    },
                    "key": "naughty_strings_fuzzy.test.v1",
                    "group": "naughty_strings"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182218574LWPU467",
            "created_at": "2023-06-18 22:18:57.098795"
        },
        "final_report": {
            "accuracy": 0.4,
            "f1_score": 0.17195790527880078
        }
    },
    "naughty_strings_graded.test.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                "openai:gpt-3.5-turbo"
            ],
            "eval_name": "naughty_strings_graded.test.v1",
            "base_eval": "naughty_strings_graded",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                    "openai:gpt-3.5-turbo"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "naughty_strings/security.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "security"
                    },
                    "key": "naughty_strings_graded.test.v1",
                    "group": "naughty_strings"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618222136PUR6TRHX",
            "created_at": "2023-06-18 22:21:36.248277"
        },
        "final_report": {
            "counts/No": 8,
            "counts/Yes": 10,
            "counts/Unsure": 2,
            "score": 0.55
        }
    },
    "number-pattern.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "number-pattern.dev.v0",
            "base_eval": "number-pattern",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "number_pattern/samples.jsonl"
                    },
                    "key": "number-pattern.dev.v0",
                    "group": "number-pattern"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618222159A5YEI2GZ",
            "created_at": "2023-06-18 22:21:59.826420"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "number-reading.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "number-reading.dev.v0",
            "base_eval": "number-reading",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "number_reading/number_reading.jsonl"
                    },
                    "key": "number-reading.dev.v0",
                    "group": "number-reading"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618222644L74CQAKT",
            "created_at": "2023-06-18 22:26:44.516004"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "partially_solved_crossword_clues.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "partially_solved_crossword_clues.dev.v0",
            "base_eval": "partially_solved_crossword_clues",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "partially_solved_crossword_clues/samples.jsonl"
                    },
                    "key": "partially_solved_crossword_clues.dev.v0",
                    "group": "partially_solved_crossword_clues"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618222739SRBYDNQB",
            "created_at": "2023-06-18 22:27:39.285173"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "pattern_identification.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "pattern_identification.dev.v0",
            "base_eval": "pattern_identification",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "pattern_identification/samples.v0.jsonl"
                    },
                    "key": "pattern_identification.dev.v0",
                    "group": "pattern_identification"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182230022O6A53VS",
            "created_at": "2023-06-18 22:30:02.594064"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "ph-calculation.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "ph-calculation.dev.v0",
            "base_eval": "ph-calculation",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "ph_calculation/samples.jsonl"
                    },
                    "key": "ph-calculation.dev.v0",
                    "group": "ph_calculation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618223308NIHKYKHY",
            "created_at": "2023-06-18 22:33:08.310962"
        },
        "final_report": {
            "accuracy": 0.05,
            "f1_score": 0.0
        }
    },
    "hand_ranks.test.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "hand_ranks.test.v1",
            "base_eval": "hand_ranks",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "poker_hand_ranks/full_samples.jsonl"
                    },
                    "key": "hand_ranks.test.v1",
                    "group": "poker_hand_ranks"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618223821GJ2BNCQP",
            "created_at": "2023-06-18 22:38:21.339814"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "regex.match.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "regex.match.dev.v0",
            "base_eval": "regex",
            "split": "match",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "regex-match/samples.jsonl"
                    },
                    "key": "regex.match.dev.v0",
                    "group": "regex-match"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182239585QXVGVGD",
            "created_at": "2023-06-18 22:39:58.766119"
        },
        "final_report": {
            "accuracy": 0.75
        }
    },
    "rot13.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "rot13.s1.simple-v0",
            "base_eval": "rot13",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "rot13/rot13.jsonl"
                    },
                    "key": "rot13.s1.simple-v0",
                    "group": "rot13"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182242092HXI5MPQ",
            "created_at": "2023-06-18 22:42:09.465304"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "rucola.test.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "rucola.test.v0",
            "base_eval": "rucola",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "rucola/samples.jsonl",
                        "few_shot_jsonl": "rucola/few_shot.jsonl",
                        "num_few_shot": 4
                    },
                    "key": "rucola.test.v0",
                    "group": "rucola"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182246365TPRUQGT",
            "created_at": "2023-06-18 22:46:36.747203"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "russe.test.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "russe.test.v0",
            "base_eval": "russe",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "russe/samples.jsonl",
                        "few_shot_jsonl": "russe/few_shot.jsonl",
                        "num_few_shot": 2
                    },
                    "key": "russe.test.v0",
                    "group": "russe"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618224810XNIO6IRW",
            "created_at": "2023-06-18 22:48:10.816421"
        },
        "final_report": {
            "accuracy": 0.7
        }
    },
    "russian-nlp-tasks.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "russian-nlp-tasks.dev.v0",
            "base_eval": "russian-nlp-tasks",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "russian-nlp-tasks/samples.jsonl"
                    },
                    "key": "russian-nlp-tasks.dev.v0",
                    "group": "russian-nlp-tasks"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618225040X7B5FALS",
            "created_at": "2023-06-18 22:50:40.397023"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "russian-rhyme.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "russian-rhyme.v0",
            "base_eval": "russian-rhyme",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "russian-rhyme/samples.jsonl"
                    },
                    "key": "russian-rhyme.v0",
                    "group": "russian-rhyme"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618225235ULAXFVA4",
            "created_at": "2023-06-18 22:52:35.027580"
        },
        "final_report": {
            "accuracy": 0.29411764705882354,
            "f1_score": 0.14183844477962126
        }
    },
    "russian_medical.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "russian_medical.dev.v0",
            "base_eval": "russian_medical",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "russian_medical/samples.jsonl"
                    },
                    "key": "russian_medical.dev.v0",
                    "group": "russian_medical"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618225458MO25GEKH",
            "created_at": "2023-06-18 22:54:58.461777"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "sarcasm.test.v1.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "sarcasm.test.v1",
            "base_eval": "sarcasm",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "sarcasm/samples.jsonl",
                        "few_shot_jsonl": "sarcasm/few_shot.jsonl",
                        "num_few_shot": 5
                    },
                    "key": "sarcasm.test.v1",
                    "group": "sarcasm"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618225550N4ZKSKZY",
            "created_at": "2023-06-18 22:55:50.764666"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "simple-knowledge-mongolian.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "simple-knowledge-mongolian.dev.v0",
            "base_eval": "simple-knowledge-mongolian",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "simple-knowledge-mongolian/samples.v0.jsonl"
                    },
                    "key": "simple-knowledge-mongolian.dev.v0",
                    "group": "simple-knowledge-mongolian"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618225707MLCKXFDP",
            "created_at": "2023-06-18 22:57:07.039962"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "sort-numbers.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "sort-numbers.s1.simple-v0",
            "base_eval": "sort-numbers",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "sort_numeric/samples.jsonl"
                    },
                    "key": "sort-numbers.s1.simple-v0",
                    "group": "sort-numeric"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618230003DBRSADX2",
            "created_at": "2023-06-18 23:00:03.216720"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "stock-options-bear-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-bear-call-spread.dev.v0",
            "base_eval": "stock-options-bear-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_bear_call_spread.jsonl"
                    },
                    "key": "stock-options-bear-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618230223KX2AEZDA",
            "created_at": "2023-06-18 23:02:23.382157"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-options-bull-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-bull-call-spread.dev.v0",
            "base_eval": "stock-options-bull-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_bull_call_spread.jsonl"
                    },
                    "key": "stock-options-bull-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618230414YG5Y5AIR",
            "created_at": "2023-06-18 23:04:14.656918"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-options-inverse-iron-butterfly-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-inverse-iron-butterfly-spread.dev.v0",
            "base_eval": "stock-options-inverse-iron-butterfly-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_inverse_iron_butterfly_spread.jsonl"
                    },
                    "key": "stock-options-inverse-iron-butterfly-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618230529BGW3OK4B",
            "created_at": "2023-06-18 23:05:29.585005"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-options-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-iron-condor-spread.dev.v0",
            "base_eval": "stock-options-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_iron_condor_spread.jsonl"
                    },
                    "key": "stock-options-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182309417BCJTBB5",
            "created_at": "2023-06-18 23:09:41.500431"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-options-inverse-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-inverse-iron-condor-spread.dev.v0",
            "base_eval": "stock-options-inverse-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_inverse_iron_condor_spread.jsonl"
                    },
                    "key": "stock-options-inverse-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182314597H4VGW3P",
            "created_at": "2023-06-18 23:14:59.541930"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-option-terms-bear-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-bear-call-spread.dev.v0",
            "base_eval": "stock-option-terms-bear-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_bear_call_spread.jsonl"
                    },
                    "key": "stock-option-terms-bear-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618231847BF7M25ZI",
            "created_at": "2023-06-18 23:18:47.274115"
        },
        "final_report": {
            "accuracy": 0.16666666666666666
        }
    },
    "stock-option-terms-bull-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-bull-call-spread.dev.v0",
            "base_eval": "stock-option-terms-bull-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_bull_call_spread.jsonl"
                    },
                    "key": "stock-option-terms-bull-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182319272PPUNVDO",
            "created_at": "2023-06-18 23:19:27.755012"
        },
        "final_report": {
            "accuracy": 0.625
        }
    },
    "stock-option-terms-iron-butterfly-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-iron-butterfly-spread.dev.v0",
            "base_eval": "stock-option-terms-iron-butterfly-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_iron_butterfly_spread.jsonl"
                    },
                    "key": "stock-option-terms-iron-butterfly-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618231948TCB7CNDL",
            "created_at": "2023-06-18 23:19:48.474366"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "stock-option-terms-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-iron-condor-spread.dev.v0",
            "base_eval": "stock-option-terms-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_iron_condor_spread.jsonl"
                    },
                    "key": "stock-option-terms-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618232043PHPZYXKN",
            "created_at": "2023-06-18 23:20:43.268367"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "stock-option-terms-inverse-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-inverse-iron-condor-spread.dev.v0",
            "base_eval": "stock-option-terms-inverse-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_inverse_iron_condor_spread.jsonl"
                    },
                    "key": "stock-option-terms-inverse-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618232134NZOAZBKD",
            "created_at": "2023-06-18 23:21:34.382989"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "swedish-spelling.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "swedish-spelling.dev.v0",
            "base_eval": "swedish-spelling",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "swedish-spelling/samples.jsonl"
                    },
                    "key": "swedish-spelling.dev.v0",
                    "group": "swedish-spelling"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618232328LZWFVNMJ",
            "created_at": "2023-06-18 23:23:28.610535"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "taxes.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "taxes.dev.v0",
            "base_eval": "taxes",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "taxes/samples.jsonl"
                    },
                    "key": "taxes.dev.v0",
                    "group": "taxes"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182325162J7ITX4J",
            "created_at": "2023-06-18 23:25:16.682147"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "tempo_to_measure_count.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "tempo_to_measure_count.dev.v0",
            "base_eval": "tempo_to_measure_count",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "tempo_to_measure_count/samples.jsonl"
                    },
                    "key": "tempo_to_measure_count.dev.v0",
                    "group": "tempo_to_measure_count"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618233008CWXNWSLN",
            "created_at": "2023-06-18 23:30:08.069489"
        },
        "final_report": {
            "accuracy": 0.3,
            "f1_score": 0.03333333333333333
        }
    },
    "test-match.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "test-match.s1.simple-v0",
            "base_eval": "test-match",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "test_match/samples.jsonl"
                    },
                    "key": "test-match.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182333563DIJ6HAF",
            "created_at": "2023-06-18 23:33:56.300732"
        },
        "final_report": {
            "accuracy": 0.6666666666666666
        }
    },
    "test-fuzzy-match.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "test-fuzzy-match.s1.simple-v0",
            "base_eval": "test-fuzzy-match",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "test_fuzzy_match/samples.jsonl"
                    },
                    "key": "test-fuzzy-match.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618233432YYX674DI",
            "created_at": "2023-06-18 23:34:32.816558"
        },
        "final_report": {
            "accuracy": 0.6666666666666666,
            "f1_score": 0.046318607764390896
        }
    },
    "test-includes.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "test-includes.s1.simple-v0",
            "base_eval": "test-includes",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.includes:Includes",
                    "args": {
                        "samples_jsonl": "test_fuzzy_match/samples.jsonl"
                    },
                    "key": "test-includes.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618233515DFKJUXSY",
            "created_at": "2023-06-18 23:35:15.561671"
        },
        "final_report": {
            "accuracy": 0.6666666666666666
        }
    },
    "test-includes-ignore-case.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "test-includes-ignore-case.s1.simple-v0",
            "base_eval": "test-includes-ignore-case",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.includes:Includes",
                    "args": {
                        "samples_jsonl": "test_fuzzy_match/samples.jsonl",
                        "ignore_case": true
                    },
                    "key": "test-includes-ignore-case.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618233549SCOZ4YHW",
            "created_at": "2023-06-18 23:35:49.273474"
        },
        "final_report": {
            "accuracy": 1.0
        }
    },
    "computer-science-problems.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "computer-science-problems.s1.simple-v0",
            "base_eval": "computer-science-problems",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "test_comp_sci/questions.jsonl"
                    },
                    "key": "computer-science-problems.s1.simple-v0",
                    "group": "test-comp-sci"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182336334KMTB26E",
            "created_at": "2023-06-18 23:36:33.503323"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "rap-people-vs-people.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                "openai:gpt-3.5-turbo"
            ],
            "eval_name": "rap-people-vs-people.dev.v0",
            "base_eval": "rap-people-vs-people",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                    "openai:gpt-3.5-turbo"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/rap_people_vs_people.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "rap-people-vs-people.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306182337567Z2KNZU3",
            "created_at": "2023-06-18 23:37:56.005003"
        },
        "final_report": {
            "counts/No": 5,
            "counts/Yes": 4,
            "score": 0.4444444444444444
        }
    },
    "rap-people-vs-fruits.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                "openai:gpt-3.5-turbo"
            ],
            "eval_name": "rap-people-vs-fruits.dev.v0",
            "base_eval": "rap-people-vs-fruits",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                    "openai:gpt-3.5-turbo"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/rap_people_vs_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "rap-people-vs-fruits.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618234521GWS63NJX",
            "created_at": "2023-06-18 23:45:21.151150"
        },
        "final_report": {
            "counts/No": 4,
            "counts/Yes": 5,
            "score": 0.5555555555555556
        }
    },
    "mg-humor-people_jp.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                "openai:gpt-3.5-turbo"
            ],
            "eval_name": "mg-humor-people_jp.dev.v0",
            "base_eval": "mg-humor-people_jp",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                    "openai:gpt-3.5-turbo"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/humor_people_jp.jsonl",
                        "eval_type": "cot_classify_jp",
                        "modelgraded_spec": "humor_jp"
                    },
                    "key": "mg-humor-people_jp.dev.v0",
                    "group": "test-modelgraded-generated"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618235245BHDZ4I5M",
            "created_at": "2023-06-18 23:52:45.295612"
        },
        "final_report": {
            "counts/1": 7,
            "counts/2": 6,
            "counts/3": 6,
            "counts/4": 1,
            "score": 2.05
        }
    },
    "diversity.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                "openai:gpt-3.5-turbo"
            ],
            "eval_name": "diversity.dev.v0",
            "base_eval": "diversity",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps",
                    "openai:gpt-3.5-turbo"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/joke_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "diversity",
                        "multicomp_n": 4,
                        "sample_kwargs": {
                            "temperature": 0.4
                        }
                    },
                    "key": "diversity.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230618235951JWJRZTAM",
            "created_at": "2023-06-18 23:59:51.178521"
        },
        "final_report": {
            "counts/Yes": 10,
            "score": 1.0
        }
    },
    "three-pt-mapping.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "three-pt-mapping.dev.v0",
            "base_eval": "three-pt-mapping",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "three-pt-mapping/three_pt_mapping.jsonl"
                    },
                    "key": "three-pt-mapping.dev.v0",
                    "group": "three-pt-mapping"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230619000619ESC7ONXR",
            "created_at": "2023-06-19 00:06:19.694494"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "ukraine-eit.val.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "ukraine-eit.val.v0",
            "base_eval": "ukraine-eit",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "ukraine_eit/samples.jsonl"
                    },
                    "key": "ukraine-eit.val.v0",
                    "group": "ukraine-eit"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2306190007526HKY6HV4",
            "created_at": "2023-06-19 00:07:52.381848"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "unified-patch.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "unified-patch.dev.v0",
            "base_eval": "unified-patch",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.includes:Includes",
                    "args": {
                        "samples_jsonl": "unified_patch/samples.jsonl"
                    },
                    "key": "unified-patch.dev.v0",
                    "group": "unified-patch"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230619001149TFEH2MCB",
            "created_at": "2023-06-19 00:11:49.498839"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "us-tort-law.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "us-tort-law.dev.v0",
            "base_eval": "us-tort-law",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "few_shot_jsonl": "us_tort_law/few_shot.jsonl",
                        "num_few_shot": 4,
                        "samples_jsonl": "us_tort_law/samples.jsonl"
                    },
                    "key": "us-tort-law.dev.v0",
                    "group": "us-tort-law"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230619001651SYKOD55G",
            "created_at": "2023-06-19 00:16:51.133485"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "utility_price_parsing.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "utility_price_parsing.dev.v0",
            "base_eval": "utility_price_parsing",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "utility_price_parsing/samples.jsonl"
                    },
                    "key": "utility_price_parsing.dev.v0",
                    "group": "utility_price_parsing"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230619002028WRDRUN2G",
            "created_at": "2023-06-19 00:20:28.579612"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "which-is-heavier.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "which-is-heavier.dev.v0",
            "base_eval": "which-is-heavier",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "which_is_heavier/which_is_heavier.jsonl"
                    },
                    "key": "which-is-heavier.dev.v0",
                    "group": "which-is-heavier"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./evaluate.py -e -m open-assistant:OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps -b openai-evals vicuna human-eval-plus",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230619002210IMSF2F3Q",
            "created_at": "2023-06-19 00:22:10.671663"
        },
        "final_report": {
            "accuracy": 0.25
        }
    }
}