{
    "forth-stack-sim-basic.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "forth-stack-sim-basic.dev.v0",
            "base_eval": "forth-stack-sim-basic",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "forth_stack_sim/basic_samples.jsonl"
                    },
                    "key": "forth-stack-sim-basic.dev.v0",
                    "group": "forth-stack-sim"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518012648AQD7T2MD",
            "created_at": "2023-05-18 01:26:48.060762"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "russian_medical.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "russian_medical.dev.v0",
            "base_eval": "russian_medical",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "russian_medical/samples.jsonl"
                    },
                    "key": "russian_medical.dev.v0",
                    "group": "russian_medical"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517211429G5NW7UVY",
            "created_at": "2023-05-17 21:14:29.201789"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-anatomy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-anatomy.val.ab-v1",
            "base_eval": "mmlu-anatomy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=anatomy&split=validation"
                    },
                    "key": "mmlu-anatomy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180222503Y772OPB",
            "created_at": "2023-05-18 02:22:50.348445"
        },
        "final_report": {
            "accuracy": 0.35714285714285715
        }
    },
    "mmlu-logical-fallacies.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-logical-fallacies.val.ab-v1",
            "base_eval": "mmlu-logical-fallacies",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=logical_fallacies&split=validation"
                    },
                    "key": "mmlu-logical-fallacies.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180412224YSN44CL",
            "created_at": "2023-05-18 04:12:22.435009"
        },
        "final_report": {
            "accuracy": 0.5555555555555556
        }
    },
    "test-includes-ignore-case.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "test-includes-ignore-case.s1.simple-v0",
            "base_eval": "test-includes-ignore-case",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.includes:Includes",
                    "args": {
                        "samples_jsonl": "test_fuzzy_match/samples.jsonl",
                        "ignore_case": true
                    },
                    "key": "test-includes-ignore-case.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517215803N2LDHAOI",
            "created_at": "2023-05-17 21:58:03.564568"
        },
        "final_report": {
            "accuracy": 0.6666666666666666
        }
    },
    "mmlu-moral-scenarios.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-moral-scenarios.val.ab-v1",
            "base_eval": "mmlu-moral-scenarios",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=moral_scenarios&split=validation"
                    },
                    "key": "mmlu-moral-scenarios.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518042835T5HCV5QB",
            "created_at": "2023-05-18 04:28:35.802226"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "russe.test.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "russe.test.v0",
            "base_eval": "russe",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "russe/samples.jsonl",
                        "few_shot_jsonl": "russe/few_shot.jsonl",
                        "num_few_shot": 2
                    },
                    "key": "russe.test.v0",
                    "group": "russe"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517225902X3VKMX2L",
            "created_at": "2023-05-17 22:59:02.865146"
        },
        "final_report": {
            "accuracy": 0.6
        }
    },
    "joke-fruits.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "joke-fruits.dev.v0",
            "base_eval": "joke-fruits",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/joke_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "humor"
                    },
                    "key": "joke-fruits.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517233854TT3V7V6K",
            "created_at": "2023-05-17 23:38:54.025993"
        },
        "final_report": {
            "counts/Unsure": 3,
            "counts/Yes": 6,
            "counts/__invalid__": 1,
            "score": 0.75
        }
    },
    "joke-animals-vs-fruits.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "joke-animals-vs-fruits.dev.v0",
            "base_eval": "joke-animals-vs-fruits",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/joke_animals_vs_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "joke-animals-vs-fruits.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23051722001837BPISOP",
            "created_at": "2023-05-17 22:00:18.960195"
        },
        "final_report": {
            "counts/No": 5,
            "counts/Yes": 4,
            "score": 0.4444444444444444
        }
    },
    "mendelian_inheritance.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mendelian_inheritance.dev.v0",
            "base_eval": "mendelian_inheritance",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "mendelian_inheritance/samples.jsonl"
                    },
                    "key": "mendelian_inheritance.dev.v0",
                    "group": "mendelian_inheritance"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305172148352F5HW45F",
            "created_at": "2023-05-17 21:48:35.172668"
        },
        "final_report": {
            "accuracy": 0.0625
        }
    },
    "which-is-heavier.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "which-is-heavier.dev.v0",
            "base_eval": "which-is-heavier",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "which_is_heavier/which_is_heavier.jsonl"
                    },
                    "key": "which-is-heavier.dev.v0",
                    "group": "which-is-heavier"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517203139XRXDSZ6K",
            "created_at": "2023-05-17 20:31:39.029341"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "coqa-fact.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "coqa-fact.dev.v0",
            "base_eval": "coqa-fact",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "coqa-fact.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517230749ASNLBHVO",
            "created_at": "2023-05-17 23:07:49.375707"
        },
        "final_report": {
            "counts/A": 3,
            "counts/B": 3,
            "counts/__invalid__": 1,
            "counts/D": 1,
            "counts/C": 1
        }
    },
    "mmlu-high-school-chemistry.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-chemistry.val.ab-v1",
            "base_eval": "mmlu-high-school-chemistry",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_chemistry&split=validation"
                    },
                    "key": "mmlu-high-school-chemistry.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518031632GRTIDPDK",
            "created_at": "2023-05-18 03:16:32.496849"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "bulgarian-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "bulgarian-lexicon.dev.v0",
            "base_eval": "bulgarian-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "bulgarian-lexicon/samples.jsonl"
                    },
                    "key": "bulgarian-lexicon.dev.v0",
                    "group": "bulgarian-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517202017PUXHEZAY",
            "created_at": "2023-05-17 20:20:17.209235"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "cube-pack.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "cube-pack.dev.v0",
            "base_eval": "cube-pack",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "cube-pack/samples.jsonl"
                    },
                    "key": "cube-pack.dev.v0",
                    "group": "cube-pack"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518003034TMCHXZ6D",
            "created_at": "2023-05-18 00:30:34.815761"
        },
        "final_report": {
            "accuracy": 0.05
        }
    },
    "mmlu-college-physics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-physics.val.ab-v1",
            "base_eval": "mmlu-college-physics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_physics&split=validation"
                    },
                    "key": "mmlu-college-physics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518024957SBGXJNLJ",
            "created_at": "2023-05-18 02:49:57.163090"
        },
        "final_report": {
            "accuracy": 0.5454545454545454
        }
    },
    "mmlu-high-school-world-history.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-world-history.val.ab-v1",
            "base_eval": "mmlu-high-school-world-history",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_world_history&split=validation"
                    },
                    "key": "mmlu-high-school-world-history.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180357304QCSZXBK",
            "created_at": "2023-05-18 03:57:30.333897"
        },
        "final_report": {
            "accuracy": 0.65
        }
    },
    "lat_long_identify.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "lat_long_identify.dev.v0",
            "base_eval": "lat_long_identify",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "lat_long_identify/samples.jsonl"
                    },
                    "key": "lat_long_identify.dev.v0",
                    "group": "lat_long_identify"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517211749ICFAEU6U",
            "created_at": "2023-05-17 21:17:49.342380"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "knot-theory-unknotting-problem.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "knot-theory-unknotting-problem.dev.v0",
            "base_eval": "knot-theory-unknotting-problem",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "knot-theory/knot-theory-unknotting-problems.jsonl"
                    },
                    "key": "knot-theory-unknotting-problem.dev.v0",
                    "group": "knot-theory"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518013359SDUDT4R3",
            "created_at": "2023-05-18 01:33:59.357309"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "joke-fruits-expl-meta.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "joke-fruits-expl-meta.dev.v0",
            "base_eval": "joke-fruits-expl-meta",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_metaeval/joke_fruits_labeled.jsonl",
                        "eval_type": "classify_cot",
                        "modelgraded_spec": "humor",
                        "metaeval": true
                    },
                    "key": "joke-fruits-expl-meta.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517234922CPXJWIQE",
            "created_at": "2023-05-17 23:49:22.047966"
        },
        "final_report": {
            "counts/Yes": 10,
            "score": 1.0,
            "metascore": 0.5
        }
    },
    "medmcqa.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "medmcqa.dev.v0",
            "base_eval": "medmcqa",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "medmcqa/samples.jsonl"
                    },
                    "key": "medmcqa.dev.v0",
                    "group": "medmcqa"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180038555NAXWX6S",
            "created_at": "2023-05-18 00:38:55.802242"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "mmlu-sociology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-sociology.val.ab-v1",
            "base_eval": "mmlu-sociology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=sociology&split=validation"
                    },
                    "key": "mmlu-sociology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518045830EODOXBTR",
            "created_at": "2023-05-18 04:58:30.277897"
        },
        "final_report": {
            "accuracy": 0.65
        }
    },
    "mmlu-us-foreign-policy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-us-foreign-policy.val.ab-v1",
            "base_eval": "mmlu-us-foreign-policy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=us_foreign_policy&split=validation"
                    },
                    "key": "mmlu-us-foreign-policy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180501256O53IXN4",
            "created_at": "2023-05-18 05:01:25.471197"
        },
        "final_report": {
            "accuracy": 0.7272727272727273
        }
    },
    "greek-vocabulary.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "greek-vocabulary.dev.v0",
            "base_eval": "greek-vocabulary",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "greek_vocabulary/samples.jsonl"
                    },
                    "key": "greek-vocabulary.dev.v0",
                    "group": "greek-vocabulary"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180052147R3QNAC7",
            "created_at": "2023-05-18 00:52:14.115621"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "knot-theory-code-conversion.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "knot-theory-code-conversion.dev.v0",
            "base_eval": "knot-theory-code-conversion",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "knot-theory/knot-theory-code-conversions.jsonl"
                    },
                    "key": "knot-theory-code-conversion.dev.v0",
                    "group": "knot-theory"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518013401IGY3HVOV",
            "created_at": "2023-05-18 01:34:01.894545"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-global-facts.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-global-facts.val.ab-v1",
            "base_eval": "mmlu-global-facts",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=global_facts&split=validation"
                    },
                    "key": "mmlu-global-facts.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518030950ZV5FJHWD",
            "created_at": "2023-05-18 03:09:50.203699"
        },
        "final_report": {
            "accuracy": 0.6
        }
    },
    "mmlu-astronomy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-astronomy.val.ab-v1",
            "base_eval": "mmlu-astronomy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=astronomy&split=validation"
                    },
                    "key": "mmlu-astronomy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518022459HEHYGDLL",
            "created_at": "2023-05-18 02:24:59.084430"
        },
        "final_report": {
            "accuracy": 0.3125
        }
    },
    "coqa-match.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "coqa-match.dev.v0",
            "base_eval": "coqa-match",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "coqa/match.jsonl"
                    },
                    "key": "coqa-match.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517230642XGKH6I5W",
            "created_at": "2023-05-17 23:06:42.257395"
        },
        "final_report": {
            "accuracy": 0.6
        }
    },
    "russian-rhyme.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "russian-rhyme.v0",
            "base_eval": "russian-rhyme",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "russian-rhyme/samples.jsonl"
                    },
                    "key": "russian-rhyme.v0",
                    "group": "russian-rhyme"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23051722330536S5EIHR",
            "created_at": "2023-05-17 22:33:05.960254"
        },
        "final_report": {
            "accuracy": 0.4117647058823529,
            "f1_score": 0.2480933531472057
        }
    },
    "japanese_driving_license.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "japanese_driving_license.s1.simple-v0",
            "base_eval": "japanese_driving_license",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "japanese_driving_license/samples.jsonl"
                    },
                    "key": "japanese_driving_license.s1.simple-v0",
                    "group": "japanese_driving_license"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517225225E33JPQ2Z",
            "created_at": "2023-05-17 22:52:25.534329"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "mmlu-human-aging.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-human-aging.val.ab-v1",
            "base_eval": "mmlu-human-aging",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=human_aging&split=validation"
                    },
                    "key": "mmlu-human-aging.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518040108TDWRVRZE",
            "created_at": "2023-05-18 04:01:08.413496"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "complex-replace-characters.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "complex-replace-characters.dev.v0",
            "base_eval": "complex-replace-characters",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "complex_replace_characters/samples.jsonl"
                    },
                    "key": "complex-replace-characters.dev.v0",
                    "group": "complex-replace-characters"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517225443KJSBXIJT",
            "created_at": "2023-05-17 22:54:43.541289"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "joke-fruits-ans-meta.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "joke-fruits-ans-meta.dev.v0",
            "base_eval": "joke-fruits-ans-meta",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_metaeval/joke_fruits_labeled.jsonl",
                        "eval_type": "classify",
                        "modelgraded_spec": "humor",
                        "metaeval": true
                    },
                    "key": "joke-fruits-ans-meta.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517235142WSBIWHOP",
            "created_at": "2023-05-17 23:51:42.183150"
        },
        "final_report": {
            "counts/Yes": 10,
            "score": 1.0,
            "metascore": 0.5
        }
    },
    "mmlu-college-biology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-biology.val.ab-v1",
            "base_eval": "mmlu-college-biology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_biology&split=validation"
                    },
                    "key": "mmlu-college-biology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518023601OHLZEAIB",
            "created_at": "2023-05-18 02:36:01.545445"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "joke-fruits-likert.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "joke-fruits-likert.dev.v0",
            "base_eval": "joke-fruits-likert",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/joke_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "humor_likert"
                    },
                    "key": "joke-fruits-likert.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517234300PEHITUMP",
            "created_at": "2023-05-17 23:43:00.433670"
        },
        "final_report": {
            "counts/__invalid__": 2,
            "counts/1": 4,
            "counts/3": 1,
            "counts/5": 2,
            "counts/2": 1,
            "score": 2.1
        }
    },
    "fcc_amateur_extra.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "fcc_amateur_extra.dev.v0",
            "base_eval": "fcc_amateur_extra",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "fcc_amateur_extra/samples.jsonl"
                    },
                    "key": "fcc_amateur_extra.dev.v0",
                    "group": "fcc_amateur_extra"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518013034HTFHDNWK",
            "created_at": "2023-05-18 01:30:34.562164"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "diagrammatic_logic.dev.v2.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "diagrammatic_logic.dev.v2",
            "base_eval": "diagrammatic_logic",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "diagrammatic_logic/samples.jsonl"
                    },
                    "key": "diagrammatic_logic.dev.v2",
                    "group": "diagrammatic_logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518000350SVUL4IT2",
            "created_at": "2023-05-18 00:03:50.130914"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "escher-sentences.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "escher-sentences.dev.v0",
            "base_eval": "escher-sentences",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "escher_sentences/samples.jsonl"
                    },
                    "key": "escher-sentences.dev.v0",
                    "group": "escher-sentences"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517201537SRG7QEOR",
            "created_at": "2023-05-17 20:15:37.325852"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "mmlu-high-school-statistics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-statistics.val.ab-v1",
            "base_eval": "mmlu-high-school-statistics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_statistics&split=validation"
                    },
                    "key": "mmlu-high-school-statistics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518034953GTTZWZEU",
            "created_at": "2023-05-18 03:49:53.294812"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "japanese-national-medical-exam01.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "japanese-national-medical-exam01.dev.v0",
            "base_eval": "japanese-national-medical-exam01",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "japanese-national-medical-exam01/japanese-national-medical-exam01.jsonl"
                    },
                    "key": "japanese-national-medical-exam01.dev.v0",
                    "group": "japanese-national-medical-exam01"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305172105486SOTUGC4",
            "created_at": "2023-05-17 21:05:48.366626"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-international-law.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-international-law.val.ab-v1",
            "base_eval": "mmlu-international-law",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=international_law&split=validation"
                    },
                    "key": "mmlu-international-law.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518040821OPL3CERY",
            "created_at": "2023-05-18 04:08:21.600198"
        },
        "final_report": {
            "accuracy": 0.6153846153846154
        }
    },
    "computer-science-problems.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "computer-science-problems.s1.simple-v0",
            "base_eval": "computer-science-problems",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "test_comp_sci/questions.jsonl"
                    },
                    "key": "computer-science-problems.s1.simple-v0",
                    "group": "test-comp-sci"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517210829I35ZJ2WP",
            "created_at": "2023-05-17 21:08:29.912658"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "logic-fact.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "logic-fact.dev.v0",
            "base_eval": "logic-fact",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "logic/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "logic-fact.dev.v0",
                    "group": "logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517215122NJKMSQA5",
            "created_at": "2023-05-17 21:51:22.788744"
        },
        "final_report": {
            "counts/C": 2,
            "counts/A": 3,
            "counts/__invalid__": 3,
            "counts/B": 1,
            "counts/D": 1
        }
    },
    "mmlu-high-school-mathematics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-mathematics.val.ab-v1",
            "base_eval": "mmlu-high-school-mathematics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_mathematics&split=validation"
                    },
                    "key": "mmlu-high-school-mathematics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518033634TOPFN4A2",
            "created_at": "2023-05-18 03:36:34.136718"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "anagrams.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "anagrams.test.v1",
            "base_eval": "anagrams",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "few_shot_jsonl": "anagrams/fewshot.jsonl",
                        "num_few_shot": 5,
                        "samples_jsonl": "anagrams/samples.jsonl"
                    },
                    "key": "anagrams.test.v1",
                    "group": "anagrams"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517212513VJ56OYNY",
            "created_at": "2023-05-17 21:25:13.247612"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "determinant.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "determinant.test.v1",
            "base_eval": "determinant",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "determinant/samples.jsonl"
                    },
                    "key": "determinant.test.v1",
                    "group": "determinant"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517233135ALMYKTYP",
            "created_at": "2023-05-17 23:31:35.144423"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "connect4.s1.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "connect4.s1.v1",
            "base_eval": "connect4",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "connect4/samples.jsonl"
                    },
                    "key": "connect4.s1.v1",
                    "group": "connect-4"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517225719J2KFZ2FI",
            "created_at": "2023-05-17 22:57:19.816030"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "categorize-with-distractors.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "categorize-with-distractors.dev.v0",
            "base_eval": "categorize-with-distractors",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "categorize_with_distractors/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "categorize-with-distractors.dev.v0",
                    "group": "categorize_with_distractors"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517204531DHRBVVZS",
            "created_at": "2023-05-17 20:45:31.196021"
        },
        "final_report": {
            "counts/E": 2,
            "counts/__invalid__": 4,
            "counts/B": 3,
            "counts/A": 5,
            "counts/D": 4,
            "counts/C": 2
        }
    },
    "number-pattern.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "number-pattern.dev.v0",
            "base_eval": "number-pattern",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "number_pattern/samples.jsonl"
                    },
                    "key": "number-pattern.dev.v0",
                    "group": "number-pattern"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518021226ZEL4IO7C",
            "created_at": "2023-05-18 02:12:26.293904"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "brazilian-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "brazilian-lexicon.dev.v0",
            "base_eval": "brazilian-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "brazilian-lexicon/samples.jsonl"
                    },
                    "key": "brazilian-lexicon.dev.v0",
                    "group": "brazilian-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518020458W3HOAMVO",
            "created_at": "2023-05-18 02:04:58.844426"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "infiniteloop-match.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "infiniteloop-match.s1.simple-v0",
            "base_eval": "infiniteloop-match",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "infiniteloop-match/infiniteloop-match.jsonl"
                    },
                    "key": "infiniteloop-match.s1.simple-v0",
                    "group": "infiniteloop-match"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518051238DIM2LZ7Q",
            "created_at": "2023-05-18 05:12:38.219499"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "loss-logic-fact.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "loss-logic-fact.dev.v0",
            "base_eval": "loss-logic-fact",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "loss_logic/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "loss-logic-fact.dev.v0",
                    "group": "loss-logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518021536HSJ7GL3J",
            "created_at": "2023-05-18 02:15:36.436206"
        },
        "final_report": {
            "counts/A": 5
        }
    },
    "mmlu-high-school-biology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-biology.val.ab-v1",
            "base_eval": "mmlu-high-school-biology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_biology&split=validation"
                    },
                    "key": "mmlu-high-school-biology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518031207UCPH4UCP",
            "created_at": "2023-05-18 03:12:07.909665"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "rot13.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "rot13.s1.simple-v0",
            "base_eval": "rot13",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "rot13/rot13.jsonl"
                    },
                    "key": "rot13.s1.simple-v0",
                    "group": "rot13"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517230059J2T4BQJZ",
            "created_at": "2023-05-17 23:00:59.407405"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "forth-stack-sim-detailed.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "forth-stack-sim-detailed.dev.v0",
            "base_eval": "forth-stack-sim-detailed",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "forth_stack_sim/detailed_samples.jsonl"
                    },
                    "key": "forth-stack-sim-detailed.dev.v0",
                    "group": "forth-stack-sim"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518012735GONAP7FX",
            "created_at": "2023-05-18 01:27:35.567564"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-us-history.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-us-history.val.ab-v1",
            "base_eval": "mmlu-high-school-us-history",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_us_history&split=validation"
                    },
                    "key": "mmlu-high-school-us-history.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518035354CCXET442",
            "created_at": "2023-05-18 03:53:54.300626"
        },
        "final_report": {
            "accuracy": 0.65
        }
    },
    "diversity.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "diversity.dev.v0",
            "base_eval": "diversity",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/joke_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "diversity",
                        "multicomp_n": 4,
                        "sample_kwargs": {
                            "temperature": 0.4
                        }
                    },
                    "key": "diversity.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517235340WZWUHUAP",
            "created_at": "2023-05-17 23:53:40.681609"
        },
        "final_report": {
            "counts/Yes": 10,
            "score": 1.0
        }
    },
    "naughty_strings_fuzzy.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "naughty_strings_fuzzy.test.v1",
            "base_eval": "naughty_strings_fuzzy",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "naughty_strings/samples.jsonl"
                    },
                    "key": "naughty_strings_fuzzy.test.v1",
                    "group": "naughty_strings"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518005712KRFGYMLI",
            "created_at": "2023-05-18 00:57:12.667278"
        },
        "final_report": {
            "accuracy": 0.5,
            "f1_score": 0.009303107357069969
        }
    },
    "manga-translation-bubble.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "manga-translation-bubble.dev.v0",
            "base_eval": "manga-translation-bubble",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.translate:Translate",
                    "args": {
                        "samples_jsonl": "manga-translation/bubbles.jsonl"
                    },
                    "key": "manga-translation-bubble.dev.v0",
                    "group": "manga-translation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517225046E7NJSI4J",
            "created_at": "2023-05-17 22:50:46.866208"
        },
        "final_report": {
            "accuracy": 0.0,
            "sacrebleu_score": 0.4356861921032228
        }
    },
    "pattern_identification.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "pattern_identification.dev.v0",
            "base_eval": "pattern_identification",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "pattern_identification/samples.v0.jsonl"
                    },
                    "key": "pattern_identification.dev.v0",
                    "group": "pattern_identification"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517222837IGFLQA2D",
            "created_at": "2023-05-17 22:28:37.333493"
        },
        "final_report": {
            "accuracy": 0.2
        }
    },
    "bitwise.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "bitwise.dev.v0",
            "base_eval": "bitwise",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "bitwise/samples.jsonl"
                    },
                    "key": "bitwise.dev.v0",
                    "group": "bitwise"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518020620JBIP2UM7",
            "created_at": "2023-05-18 02:06:20.825603"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "simple-knowledge-mongolian.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "simple-knowledge-mongolian.dev.v0",
            "base_eval": "simple-knowledge-mongolian",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "simple-knowledge-mongolian/samples.v0.jsonl"
                    },
                    "key": "simple-knowledge-mongolian.dev.v0",
                    "group": "simple-knowledge-mongolian"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518012329OKEGKRRK",
            "created_at": "2023-05-18 01:23:29.722686"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "test-match.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "test-match.s1.simple-v0",
            "base_eval": "test-match",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "test_match/samples.jsonl"
                    },
                    "key": "test-match.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517215558BNFZVYOW",
            "created_at": "2023-05-17 21:55:58.606715"
        },
        "final_report": {
            "accuracy": 0.6666666666666666
        }
    },
    "mmlu-college-medicine.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-medicine.val.ab-v1",
            "base_eval": "mmlu-college-medicine",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_medicine&split=validation"
                    },
                    "key": "mmlu-college-medicine.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518024542DMHTW7NL",
            "created_at": "2023-05-18 02:45:42.475572"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "illinois-law.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "illinois-law.v0",
            "base_eval": "illinois-law",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "illinois-law/samples.jsonl"
                    },
                    "key": "illinois-law.v0",
                    "group": "illinois-law"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517203833GEUZHJIV",
            "created_at": "2023-05-17 20:38:33.301180"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "mmlu-abstract-algebra.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-abstract-algebra.val.ab-v1",
            "base_eval": "mmlu-abstract-algebra",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=abstract_algebra&split=validation"
                    },
                    "key": "mmlu-abstract-algebra.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518022042QK3K5LKI",
            "created_at": "2023-05-18 02:20:42.006734"
        },
        "final_report": {
            "accuracy": 0.18181818181818182
        }
    },
    "mmlu-high-school-physics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-physics.val.ab-v1",
            "base_eval": "mmlu-high-school-physics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_physics&split=validation"
                    },
                    "key": "mmlu-high-school-physics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518034345DIVCLWJT",
            "created_at": "2023-05-18 03:43:45.778300"
        },
        "final_report": {
            "accuracy": 0.35294117647058826
        }
    },
    "unified-patch.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "unified-patch.dev.v0",
            "base_eval": "unified-patch",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.includes:Includes",
                    "args": {
                        "samples_jsonl": "unified_patch/samples.jsonl"
                    },
                    "key": "unified-patch.dev.v0",
                    "group": "unified-patch"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518002452I33JJDUQ",
            "created_at": "2023-05-18 00:24:52.152659"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-government-and-politics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-government-and-politics.val.ab-v1",
            "base_eval": "mmlu-high-school-government-and-politics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_government_and_politics&split=validation"
                    },
                    "key": "mmlu-high-school-government-and-politics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518032849HVGRMOXV",
            "created_at": "2023-05-18 03:28:49.052164"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "heart-disease.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "heart-disease.v0",
            "base_eval": "heart-disease",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "heart-disease/samples.jsonl"
                    },
                    "key": "heart-disease.v0",
                    "group": "heart-disease"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518003211BHGUNG6K",
            "created_at": "2023-05-18 00:32:11.735700"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "stock-options-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-iron-condor-spread.dev.v0",
            "base_eval": "stock-options-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_iron_condor_spread.jsonl"
                    },
                    "key": "stock-options-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517213623743JQDLZ",
            "created_at": "2023-05-17 21:36:23.250942"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "naughty_strings_graded.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "naughty_strings_graded.test.v1",
            "base_eval": "naughty_strings_graded",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "naughty_strings/security.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "security"
                    },
                    "key": "naughty_strings_graded.test.v1",
                    "group": "naughty_strings"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518010108HAAQ2HDQ",
            "created_at": "2023-05-18 01:01:08.301482"
        },
        "final_report": {
            "counts/Yes": 8,
            "counts/Unsure": 4,
            "counts/No": 5,
            "counts/__invalid__": 3,
            "score": 0.5
        }
    },
    "manga-translation-page.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "manga-translation-page.dev.v0",
            "base_eval": "manga-translation-page",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.translate:Translate",
                    "args": {
                        "samples_jsonl": "manga-translation/pages.jsonl"
                    },
                    "key": "manga-translation-page.dev.v0",
                    "group": "manga-translation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305172243273FEI2YCT",
            "created_at": "2023-05-17 22:43:27.861289"
        },
        "final_report": {
            "accuracy": 0.0,
            "sacrebleu_score": 1.9656282303766779
        }
    },
    "formal-logic.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "formal-logic.dev.v0",
            "base_eval": "formal-logic",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "formal_logic/formal_logic_expressions.jsonl"
                    },
                    "key": "formal-logic.dev.v0",
                    "group": "formal_logic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518004510Q7H654L2",
            "created_at": "2023-05-18 00:45:10.220112"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "ukraine-eit.val.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "ukraine-eit.val.v0",
            "base_eval": "ukraine-eit",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "ukraine_eit/samples.jsonl"
                    },
                    "key": "ukraine-eit.val.v0",
                    "group": "ukraine-eit"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518011520OM2BLUCX",
            "created_at": "2023-05-18 01:15:20.330541"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mg-humor-people_jp.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mg-humor-people_jp.dev.v0",
            "base_eval": "mg-humor-people_jp",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_modelgraded/humor_people_jp.jsonl",
                        "eval_type": "cot_classify_jp",
                        "modelgraded_spec": "humor_jp"
                    },
                    "key": "mg-humor-people_jp.dev.v0",
                    "group": "test-modelgraded-generated"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180151395LWLY3XK",
            "created_at": "2023-05-18 01:51:39.237696"
        },
        "final_report": {
            "counts/__invalid__": 9,
            "counts/5": 3,
            "counts/4": 3,
            "counts/1": 2,
            "counts/2": 2,
            "counts/3": 1,
            "score": 2.25
        }
    },
    "mmlu-business-ethics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-business-ethics.val.ab-v1",
            "base_eval": "mmlu-business-ethics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=business_ethics&split=validation"
                    },
                    "key": "mmlu-business-ethics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180229335PRVWP5Q",
            "created_at": "2023-05-18 02:29:33.365106"
        },
        "final_report": {
            "accuracy": 0.45454545454545453
        }
    },
    "born-first.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "born-first.dev.v0",
            "base_eval": "born-first",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "born_first/born_first.jsonl"
                    },
                    "key": "born-first.dev.v0",
                    "group": "born-first"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518011126CL2BRHBX",
            "created_at": "2023-05-18 01:11:26.922406"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "mmlu-professional-psychology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-professional-psychology.val.ab-v1",
            "base_eval": "mmlu-professional-psychology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_psychology&split=validation"
                    },
                    "key": "mmlu-professional-psychology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518045043VDSFYB4R",
            "created_at": "2023-05-18 04:50:43.721832"
        },
        "final_report": {
            "accuracy": 0.75
        }
    },
    "mmlu-human-sexuality.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-human-sexuality.val.ab-v1",
            "base_eval": "mmlu-human-sexuality",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=human_sexuality&split=validation"
                    },
                    "key": "mmlu-human-sexuality.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23051804052825LIZVXJ",
            "created_at": "2023-05-18 04:05:28.484395"
        },
        "final_report": {
            "accuracy": 0.4166666666666667
        }
    },
    "test-fuzzy-match.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "test-fuzzy-match.s1.simple-v0",
            "base_eval": "test-fuzzy-match",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "test_fuzzy_match/samples.jsonl"
                    },
                    "key": "test-fuzzy-match.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517215628EK5C3MMF",
            "created_at": "2023-05-17 21:56:28.873801"
        },
        "final_report": {
            "accuracy": 0.6666666666666666,
            "f1_score": 0.04226197264171947
        }
    },
    "rap-animals-vs-fruits.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "rap-animals-vs-fruits.dev.v0",
            "base_eval": "rap-animals-vs-fruits",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/rap_animals_vs_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "rap-animals-vs-fruits.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517221314QMBAACZ7",
            "created_at": "2023-05-17 22:13:14.539725"
        },
        "final_report": {
            "counts/Yes": 9,
            "score": 1.0
        }
    },
    "bigrams.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "bigrams.dev.v0",
            "base_eval": "bigrams",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "bigrams/samples.jsonl"
                    },
                    "key": "bigrams.dev.v0",
                    "group": "bigrams"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517202138U2SRBFAP",
            "created_at": "2023-05-17 20:21:38.552528"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "hindi_upsc.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "hindi_upsc.dev.v0",
            "base_eval": "hindi_upsc",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "hindi_upsc/samples.jsonl"
                    },
                    "key": "hindi_upsc.dev.v0",
                    "group": "hindi_upsc"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517202426JFSLR776",
            "created_at": "2023-05-17 20:24:26.145669"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "naughty_strings.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "naughty_strings.test.v1",
            "base_eval": "naughty_strings",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "naughty_strings/samples.jsonl"
                    },
                    "key": "naughty_strings.test.v1",
                    "group": "naughty_strings"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518005341OGLYMPRC",
            "created_at": "2023-05-18 00:53:41.288447"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-professional-accounting.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-professional-accounting.val.ab-v1",
            "base_eval": "mmlu-professional-accounting",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_accounting&split=validation"
                    },
                    "key": "mmlu-professional-accounting.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518044052TGQQZ7MN",
            "created_at": "2023-05-18 04:40:52.746899"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "joke-fruits-meta.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "joke-fruits-meta.dev.v0",
            "base_eval": "joke-fruits-meta",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_metaeval/joke_fruits_labeled.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "humor",
                        "metaeval": true
                    },
                    "key": "joke-fruits-meta.dev.v0",
                    "group": "test-modelgraded"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517234709RGA6AEVB",
            "created_at": "2023-05-17 23:47:09.381850"
        },
        "final_report": {
            "counts/Unsure": 2,
            "counts/No": 1,
            "counts/Yes": 7,
            "score": 0.8,
            "metascore": 0.4
        }
    },
    "mmlu-high-school-computer-science.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-computer-science.val.ab-v1",
            "base_eval": "mmlu-high-school-computer-science",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_computer_science&split=validation"
                    },
                    "key": "mmlu-high-school-computer-science.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180320585T4KRUG5",
            "created_at": "2023-05-18 03:20:58.792928"
        },
        "final_report": {
            "accuracy": 0.5555555555555556
        }
    },
    "sort-numbers.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "sort-numbers.s1.simple-v0",
            "base_eval": "sort-numbers",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "sort_numeric/samples.jsonl"
                    },
                    "key": "sort-numbers.s1.simple-v0",
                    "group": "sort-numeric"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305172159396EW3ZT2Q",
            "created_at": "2023-05-17 21:59:39.035132"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "chess-piece-count.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "chess-piece-count.s1.simple-v0",
            "base_eval": "chess-piece-count",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "chess_piece_count/fuzzy_match.jsonl"
                    },
                    "key": "chess-piece-count.s1.simple-v0",
                    "group": "chess-piece-count"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517211048KN6APPBG",
            "created_at": "2023-05-17 21:10:48.754591"
        },
        "final_report": {
            "accuracy": 0.05,
            "f1_score": 0.0007751937984496124
        }
    },
    "tempo_to_measure_count.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "tempo_to_measure_count.dev.v0",
            "base_eval": "tempo_to_measure_count",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "tempo_to_measure_count/samples.jsonl"
                    },
                    "key": "tempo_to_measure_count.dev.v0",
                    "group": "tempo_to_measure_count"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518014517IWK25FOO",
            "created_at": "2023-05-18 01:45:17.860351"
        },
        "final_report": {
            "accuracy": 0.3,
            "f1_score": 0.0005405405405405405
        }
    },
    "coqa-fact-expl.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "coqa-fact-expl.dev.v0",
            "base_eval": "coqa-fact-expl",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "eval_type": "classify_cot",
                        "modelgraded_spec": "fact"
                    },
                    "key": "coqa-fact-expl.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517231217OOIVSD2T",
            "created_at": "2023-05-17 23:12:17.953719"
        },
        "final_report": {
            "counts/C": 6,
            "counts/A": 1,
            "counts/B": 2
        }
    },
    "coqa-closedqa-correct.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "coqa-closedqa-correct.dev.v0",
            "base_eval": "coqa-closedqa-correct",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "modelgraded_spec": "closedqa",
                        "modelgraded_spec_args": {
                            "criteria": "correctness: Is the answer correct?"
                        }
                    },
                    "key": "coqa-closedqa-correct.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517231506WIA5ZUNX",
            "created_at": "2023-05-17 23:15:06.626112"
        },
        "final_report": {
            "counts/Y": 6,
            "counts/N": 2,
            "counts/__invalid__": 1,
            "score": 0.6666666666666666
        }
    },
    "naughty_strings_graded_meta.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "naughty_strings_graded_meta.test.v1",
            "base_eval": "naughty_strings_graded_meta",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "naughty_strings/security.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "security",
                        "metaeval": true
                    },
                    "key": "naughty_strings_graded_meta.test.v1",
                    "group": "naughty_strings"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518010640T72KCBJR",
            "created_at": "2023-05-18 01:06:40.549674"
        },
        "final_report": {
            "counts/Unsure": 5,
            "counts/No": 5,
            "counts/__invalid__": 6,
            "counts/Yes": 4,
            "score": 0.325,
            "metascore": 0.0
        }
    },
    "mmlu-formal-logic.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-formal-logic.val.ab-v1",
            "base_eval": "mmlu-formal-logic",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=formal_logic&split=validation"
                    },
                    "key": "mmlu-formal-logic.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180306506UJVZUGV",
            "created_at": "2023-05-18 03:06:50.671244"
        },
        "final_report": {
            "accuracy": 0.42857142857142855
        }
    },
    "stock-option-terms-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-iron-condor-spread.dev.v0",
            "base_eval": "stock-option-terms-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_iron_condor_spread.jsonl"
                    },
                    "key": "stock-option-terms-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517214537I2S5SL5I",
            "created_at": "2023-05-17 21:45:37.334669"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "logic-liar-paradox.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "logic-liar-paradox.dev.v0",
            "base_eval": "logic-liar-paradox",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "logic-liar-paradox/samples.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "fact"
                    },
                    "key": "logic-liar-paradox.dev.v0",
                    "group": "logic-liar-paradox"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305172053234F4GIDQY",
            "created_at": "2023-05-17 20:53:23.806040"
        },
        "final_report": {
            "counts/A": 5,
            "counts/D": 1,
            "counts/B": 5,
            "counts/E": 3,
            "counts/C": 1
        }
    },
    "aba_mrpc_true_false.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "aba_mrpc_true_false.dev.v0",
            "base_eval": "aba_mrpc_true_false",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "aba_mrpc_true_false/samples.jsonl"
                    },
                    "key": "aba_mrpc_true_false.dev.v0",
                    "group": "aba-mrpc-true-false"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517230338OS7MQSCT",
            "created_at": "2023-05-17 23:03:38.763374"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "balance-chemical-equation.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "balance-chemical-equation.dev.v0",
            "base_eval": "balance-chemical-equation",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "balance_chemical_equation/samples.jsonl"
                    },
                    "key": "balance-chemical-equation.dev.v0",
                    "group": "balance-chemical-equation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517204234IFNLY5O3",
            "created_at": "2023-05-17 20:42:34.269352"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "chess.match.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "chess.match.dev.v0",
            "base_eval": "chess",
            "split": "match",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "chess/match.jsonl"
                    },
                    "key": "chess.match.dev.v0",
                    "group": "chess"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517210239PB7HC773",
            "created_at": "2023-05-17 21:02:39.702866"
        },
        "final_report": {
            "accuracy": 0.5,
            "f1_score": 0.014112323183888756
        }
    },
    "stock-options-bear-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-bear-call-spread.dev.v0",
            "base_eval": "stock-options-bear-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_bear_call_spread.jsonl"
                    },
                    "key": "stock-options-bear-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517212916BHCN2XLT",
            "created_at": "2023-05-17 21:29:16.613026"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-professional-law.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-professional-law.val.ab-v1",
            "base_eval": "mmlu-professional-law",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_law&split=validation"
                    },
                    "key": "mmlu-professional-law.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518044444OWRP45RV",
            "created_at": "2023-05-18 04:44:44.594244"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "decrypt-caesar-cipher.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "decrypt-caesar-cipher.dev.v0",
            "base_eval": "decrypt-caesar-cipher",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "decrypt_caesar_cipher/samples.jsonl"
                    },
                    "key": "decrypt-caesar-cipher.dev.v0",
                    "group": "decrypt-caesar-cipher"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517233353HVEEGVVW",
            "created_at": "2023-05-17 23:33:53.027669"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "us-tort-law.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "us-tort-law.dev.v0",
            "base_eval": "us-tort-law",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "few_shot_jsonl": "us_tort_law/few_shot.jsonl",
                        "num_few_shot": 4,
                        "samples_jsonl": "us_tort_law/samples.jsonl"
                    },
                    "key": "us-tort-law.dev.v0",
                    "group": "us-tort-law"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518004612UMKIYEDA",
            "created_at": "2023-05-18 00:46:12.259946"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "hebrew-rhyme.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "hebrew-rhyme.v0",
            "base_eval": "hebrew-rhyme",
            "split": "v0",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "hebrew_rhyme/samples.jsonl"
                    },
                    "key": "hebrew-rhyme.v0",
                    "group": "hebrew-rhyme"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517211521BOAONIOI",
            "created_at": "2023-05-17 21:15:21.217184"
        },
        "final_report": {
            "accuracy": 0.1,
            "f1_score": 0.05299373040752351
        }
    },
    "mmlu-computer-security.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-computer-security.val.ab-v1",
            "base_eval": "mmlu-computer-security",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=computer_security&split=validation"
                    },
                    "key": "mmlu-computer-security.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518025143IOSDN32G",
            "created_at": "2023-05-18 02:51:43.134863"
        },
        "final_report": {
            "accuracy": 0.45454545454545453
        }
    },
    "taxes.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "taxes.dev.v0",
            "base_eval": "taxes",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "taxes/samples.jsonl"
                    },
                    "key": "taxes.dev.v0",
                    "group": "taxes"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23051722350177LH6DFG",
            "created_at": "2023-05-17 22:35:01.750030"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "mmlu-philosophy.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-philosophy.val.ab-v1",
            "base_eval": "mmlu-philosophy",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=philosophy&split=validation"
                    },
                    "key": "mmlu-philosophy.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518043434ZUJATSBR",
            "created_at": "2023-05-18 04:34:34.720115"
        },
        "final_report": {
            "accuracy": 0.65
        }
    },
    "mmlu-college-mathematics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-mathematics.val.ab-v1",
            "base_eval": "mmlu-college-mathematics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_mathematics&split=validation"
                    },
                    "key": "mmlu-college-mathematics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518024330QY5C2ORF",
            "created_at": "2023-05-18 02:43:30.211200"
        },
        "final_report": {
            "accuracy": 0.18181818181818182
        }
    },
    "manga-translation-panel.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "manga-translation-panel.dev.v0",
            "base_eval": "manga-translation-panel",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.translate:Translate",
                    "args": {
                        "samples_jsonl": "manga-translation/panels.jsonl"
                    },
                    "key": "manga-translation-panel.dev.v0",
                    "group": "manga-translation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517224815DKEDEFLW",
            "created_at": "2023-05-17 22:48:15.962498"
        },
        "final_report": {
            "accuracy": 0.05,
            "sacrebleu_score": 0.48022538704955753
        }
    },
    "stock-option-terms-bull-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-bull-call-spread.dev.v0",
            "base_eval": "stock-option-terms-bull-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_bull_call_spread.jsonl"
                    },
                    "key": "stock-option-terms-bull-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517214320IUUTWHJH",
            "created_at": "2023-05-17 21:43:20.521509"
        },
        "final_report": {
            "accuracy": 0.625
        }
    },
    "match_banking77.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "match_banking77.test.v1",
            "base_eval": "match_banking77",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "banking77/samples.jsonl"
                    },
                    "key": "match_banking77.test.v1",
                    "group": "banking77"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180032522S34LNXA",
            "created_at": "2023-05-18 00:32:52.590407"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "coqa-closedqa-conciseness.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "coqa-closedqa-conciseness.dev.v0",
            "base_eval": "coqa-closedqa-conciseness",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "modelgraded_spec": "closedqa",
                        "modelgraded_spec_args": {
                            "criteria": "conciseness: Is the answer concise and to the point?"
                        }
                    },
                    "key": "coqa-closedqa-conciseness.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517232245QJDCSJDH",
            "created_at": "2023-05-17 23:22:45.090317"
        },
        "final_report": {
            "counts/Y": 5,
            "counts/N": 3,
            "counts/__invalid__": 1,
            "score": 0.5555555555555556
        }
    },
    "mmlu-public-relations.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-public-relations.val.ab-v1",
            "base_eval": "mmlu-public-relations",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=public_relations&split=validation"
                    },
                    "key": "mmlu-public-relations.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180453353DDOLJQE",
            "created_at": "2023-05-18 04:53:35.006865"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "last-word-nth.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "last-word-nth.s1.simple-v0",
            "base_eval": "last-word-nth",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "last_word_nth/samples.jsonl"
                    },
                    "key": "last-word-nth.s1.simple-v0",
                    "group": "last-word-nth"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518021749WCCPJHNA",
            "created_at": "2023-05-18 02:17:49.417190"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "emotional-intelligence.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "emotional-intelligence.dev.v0",
            "base_eval": "emotional-intelligence",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "emotional-intelligence/samples.jsonl"
                    },
                    "key": "emotional-intelligence.dev.v0",
                    "group": "emotional-intelligence"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517202645YRQP4I7G",
            "created_at": "2023-05-17 20:26:45.899382"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "ph-calculation.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "ph-calculation.dev.v0",
            "base_eval": "ph-calculation",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "ph_calculation/samples.jsonl"
                    },
                    "key": "ph-calculation.dev.v0",
                    "group": "ph_calculation"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180040254C65DVUY",
            "created_at": "2023-05-18 00:40:25.094066"
        },
        "final_report": {
            "accuracy": 0.15,
            "f1_score": 0.0
        }
    },
    "mmlu-elementary-mathematics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-elementary-mathematics.val.ab-v1",
            "base_eval": "mmlu-elementary-mathematics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=elementary_mathematics&split=validation"
                    },
                    "key": "mmlu-elementary-mathematics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518030323VFA5XA6W",
            "created_at": "2023-05-18 03:03:23.920293"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "logiqa.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "logiqa.dev.v0",
            "base_eval": "logiqa",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "logiqa/logiqa.jsonl"
                    },
                    "key": "logiqa.dev.v0",
                    "group": "logiqa"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517203436YDAOPMDP",
            "created_at": "2023-05-17 20:34:36.758665"
        },
        "final_report": {
            "accuracy": 0.15
        }
    },
    "belarusian-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "belarusian-lexicon.dev.v0",
            "base_eval": "belarusian-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "belarusian_lexicon/samples.jsonl"
                    },
                    "key": "belarusian-lexicon.dev.v0",
                    "group": "belarusian-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517204434NVHVMHP6",
            "created_at": "2023-05-17 20:44:34.664080"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "rucola.test.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "rucola.test.v0",
            "base_eval": "rucola",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "rucola/samples.jsonl",
                        "few_shot_jsonl": "rucola/few_shot.jsonl",
                        "num_few_shot": 4
                    },
                    "key": "rucola.test.v0",
                    "group": "rucola"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518014324ADWJNK4N",
            "created_at": "2023-05-18 01:43:24.681728"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "mmlu-prehistory.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-prehistory.val.ab-v1",
            "base_eval": "mmlu-prehistory",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=prehistory&split=validation"
                    },
                    "key": "mmlu-prehistory.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518043722G6AQ6HZS",
            "created_at": "2023-05-18 04:37:22.272117"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "three-pt-mapping.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "three-pt-mapping.dev.v0",
            "base_eval": "three-pt-mapping",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "three-pt-mapping/three_pt_mapping.jsonl"
                    },
                    "key": "three-pt-mapping.dev.v0",
                    "group": "three-pt-mapping"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517215902RTKM4WF3",
            "created_at": "2023-05-17 21:59:02.605534"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-high-school-microeconomics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-microeconomics.val.ab-v1",
            "base_eval": "mmlu-high-school-microeconomics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_microeconomics&split=validation"
                    },
                    "key": "mmlu-high-school-microeconomics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518033912C5RDCXSP",
            "created_at": "2023-05-18 03:39:12.792438"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "mmlu-moral-disputes.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-moral-disputes.val.ab-v1",
            "base_eval": "mmlu-moral-disputes",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=moral_disputes&split=validation"
                    },
                    "key": "mmlu-moral-disputes.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180425434UMK3YMX",
            "created_at": "2023-05-18 04:25:43.750567"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "coqa-closedqa-relevance.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "coqa-closedqa-relevance.dev.v0",
            "base_eval": "coqa-closedqa-relevance",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "coqa/samples.jsonl",
                        "modelgraded_spec": "closedqa",
                        "modelgraded_spec_args": {
                            "criteria": "relevance: Is the submission referring to a real quote from the text?"
                        }
                    },
                    "key": "coqa-closedqa-relevance.dev.v0",
                    "group": "coqa-ex"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23051723180267RYAKO3",
            "created_at": "2023-05-17 23:18:02.397390"
        },
        "final_report": {
            "counts/Y": 8,
            "counts/N": 1,
            "score": 0.8888888888888888
        }
    },
    "hellaswag.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "hellaswag.val.ab-v1",
            "base_eval": "hellaswag",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://hellaswag?split=validation",
                        "instructions": "Choose the most plausible continuation for the story."
                    },
                    "key": "hellaswag.val.ab-v1",
                    "group": "language"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518051407HVB5GAIQ",
            "created_at": "2023-05-18 05:14:07.969516"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "mmlu-high-school-geography.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-geography.val.ab-v1",
            "base_eval": "mmlu-high-school-geography",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_geography&split=validation"
                    },
                    "key": "mmlu-high-school-geography.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518032609SLEDADLE",
            "created_at": "2023-05-18 03:26:09.989177"
        },
        "final_report": {
            "accuracy": 0.6
        }
    },
    "mmlu-virology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-virology.val.ab-v1",
            "base_eval": "mmlu-virology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=virology&split=validation"
                    },
                    "key": "mmlu-virology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518050402X5YUOMLJ",
            "created_at": "2023-05-18 05:04:02.033522"
        },
        "final_report": {
            "accuracy": 0.5
        }
    },
    "regex.match.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "regex.match.dev.v0",
            "base_eval": "regex",
            "split": "match",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "regex-match/samples.jsonl"
                    },
                    "key": "regex.match.dev.v0",
                    "group": "regex-match"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518004846DLLVERMN",
            "created_at": "2023-05-18 00:48:46.900320"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "russian-nlp-tasks.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "russian-nlp-tasks.dev.v0",
            "base_eval": "russian-nlp-tasks",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "russian-nlp-tasks/samples.jsonl"
                    },
                    "key": "russian-nlp-tasks.dev.v0",
                    "group": "russian-nlp-tasks"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518014802YBNIOT2N",
            "created_at": "2023-05-18 01:48:02.054743"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "knot-theory-unknotting-number.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "knot-theory-unknotting-number.dev.v0",
            "base_eval": "knot-theory-unknotting-number",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "knot-theory/knot-theory-unknotting-numbers.jsonl"
                    },
                    "key": "knot-theory-unknotting-number.dev.v0",
                    "group": "knot-theory"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518013945HJZ4JMV4",
            "created_at": "2023-05-18 01:39:45.509466"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "stock-option-terms-bear-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-bear-call-spread.dev.v0",
            "base_eval": "stock-option-terms-bear-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_bear_call_spread.jsonl"
                    },
                    "key": "stock-option-terms-bear-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517214239FYUJF3RH",
            "created_at": "2023-05-17 21:42:39.372351"
        },
        "final_report": {
            "accuracy": 0.08333333333333333
        }
    },
    "dice-rotation-sequence.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "dice-rotation-sequence.dev.v0",
            "base_eval": "dice-rotation-sequence",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "dice-rotation-sequence/samples.jsonl"
                    },
                    "key": "dice-rotation-sequence.dev.v0",
                    "group": "dice-rotation-sequence"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517205224GSWIXFGS",
            "created_at": "2023-05-17 20:52:24.333795"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "finance.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "finance.dev.v0",
            "base_eval": "finance",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "finance/credit.jsonl"
                    },
                    "key": "finance.dev.v0",
                    "group": "finance"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518003721SULIRLVP",
            "created_at": "2023-05-18 00:37:21.783708"
        },
        "final_report": {
            "accuracy": 0.0,
            "f1_score": 0.0
        }
    },
    "mmlu-econometrics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-econometrics.val.ab-v1",
            "base_eval": "mmlu-econometrics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=econometrics&split=validation"
                    },
                    "key": "mmlu-econometrics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518025735YMNWCXVH",
            "created_at": "2023-05-18 02:57:35.804038"
        },
        "final_report": {
            "accuracy": 0.3333333333333333
        }
    },
    "mmlu-miscellaneous.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-miscellaneous.val.ab-v1",
            "base_eval": "mmlu-miscellaneous",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=miscellaneous&split=validation"
                    },
                    "key": "mmlu-miscellaneous.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518042340QZVEI6WE",
            "created_at": "2023-05-18 04:23:40.746454"
        },
        "final_report": {
            "accuracy": 0.6
        }
    },
    "mmlu-marketing.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-marketing.val.ab-v1",
            "base_eval": "mmlu-marketing",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=marketing&split=validation"
                    },
                    "key": "mmlu-marketing.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518041945F3DCGLXP",
            "created_at": "2023-05-18 04:19:45.038337"
        },
        "final_report": {
            "accuracy": 0.8
        }
    },
    "number-reading.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "number-reading.dev.v0",
            "base_eval": "number-reading",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "number_reading/number_reading.jsonl"
                    },
                    "key": "number-reading.dev.v0",
                    "group": "number-reading"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180009257AQYSM2X",
            "created_at": "2023-05-18 00:09:25.545392"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "forth-stack-sim.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "forth-stack-sim.dev.v0",
            "base_eval": "forth-stack-sim",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "forth_stack_sim/samples.jsonl"
                    },
                    "key": "forth-stack-sim.dev.v0",
                    "group": "forth-stack-sim"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518012531LHSLEJEE",
            "created_at": "2023-05-18 01:25:31.589352"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "actors-sequence.dev.match-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "actors-sequence.dev.match-v1",
            "base_eval": "actors-sequence",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "actors-sequence/samples.jsonl"
                    },
                    "key": "actors-sequence.dev.match-v1",
                    "group": "actors-sequence"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517212128IAAXWDLR",
            "created_at": "2023-05-17 21:21:28.263757"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "dutch-lexicon.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "dutch-lexicon.dev.v0",
            "base_eval": "dutch-lexicon",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "dutch-lexicon/samples.jsonl"
                    },
                    "key": "dutch-lexicon.dev.v0",
                    "group": "dutch-lexicon"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180005557P7GKSYN",
            "created_at": "2023-05-18 00:05:55.554455"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "mmlu-security-studies.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-security-studies.val.ab-v1",
            "base_eval": "mmlu-security-studies",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=security_studies&split=validation"
                    },
                    "key": "mmlu-security-studies.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518045520IZHP7RA2",
            "created_at": "2023-05-18 04:55:20.184439"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "qa.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "qa.dev.v0",
            "base_eval": "qa",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "qa/q_and_a.jsonl"
                    },
                    "key": "qa.dev.v0",
                    "group": "qa"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518000956MSVKRZMK",
            "created_at": "2023-05-18 00:09:56.474987"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "utility_price_parsing.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "utility_price_parsing.dev.v0",
            "base_eval": "utility_price_parsing",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "utility_price_parsing/samples.jsonl"
                    },
                    "key": "utility_price_parsing.dev.v0",
                    "group": "utility_price_parsing"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517233757HI7TSER2",
            "created_at": "2023-05-17 23:37:57.325353"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-electrical-engineering.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-electrical-engineering.val.ab-v1",
            "base_eval": "mmlu-electrical-engineering",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=electrical_engineering&split=validation"
                    },
                    "key": "mmlu-electrical-engineering.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518030046OTTUHOQ6",
            "created_at": "2023-05-18 03:00:46.184345"
        },
        "final_report": {
            "accuracy": 0.1875
        }
    },
    "algebra-word-problems.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "algebra-word-problems.s1.simple-v0",
            "base_eval": "algebra-word-problems",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "algebra_word_problems/samples.jsonl"
                    },
                    "key": "algebra-word-problems.s1.simple-v0",
                    "group": "algebra-word-problems"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305172231562T72FTGW",
            "created_at": "2023-05-17 22:31:56.841932"
        },
        "final_report": {
            "accuracy": 0.3333333333333333
        }
    },
    "compare-countries-area.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "compare-countries-area.dev.v0",
            "base_eval": "compare-countries-area",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "compare-countries-area/samples.jsonl"
                    },
                    "key": "compare-countries-area.dev.v0",
                    "group": "compare-countries-area"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518003415FPBRHK7L",
            "created_at": "2023-05-18 00:34:15.022012"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "mmlu-high-school-macroeconomics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-macroeconomics.val.ab-v1",
            "base_eval": "mmlu-high-school-macroeconomics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_macroeconomics&split=validation"
                    },
                    "key": "mmlu-high-school-macroeconomics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518033208GY2FPJHQ",
            "created_at": "2023-05-18 03:32:08.620205"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "sarcasm.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "sarcasm.test.v1",
            "base_eval": "sarcasm",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "sarcasm/samples.jsonl",
                        "few_shot_jsonl": "sarcasm/few_shot.jsonl",
                        "num_few_shot": 5
                    },
                    "key": "sarcasm.test.v1",
                    "group": "sarcasm"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518000210IN6TDR25",
            "created_at": "2023-05-18 00:02:10.597140"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "mmlu-college-chemistry.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-chemistry.val.ab-v1",
            "base_eval": "mmlu-college-chemistry",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_chemistry&split=validation"
                    },
                    "key": "mmlu-college-chemistry.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518023959SB5XNALK",
            "created_at": "2023-05-18 02:39:59.020515"
        },
        "final_report": {
            "accuracy": 0.125
        }
    },
    "reverse-string.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "reverse-string.s1.simple-v0",
            "base_eval": "reverse-string",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "reverse_string/reverse_string.jsonl"
                    },
                    "key": "reverse-string.s1.simple-v0",
                    "group": "reverse-string"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305172227043G4DHU3U",
            "created_at": "2023-05-17 22:27:04.602468"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-college-computer-science.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-college-computer-science.val.ab-v1",
            "base_eval": "mmlu-college-computer-science",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=college_computer_science&split=validation"
                    },
                    "key": "mmlu-college-computer-science.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180241487S3RKJSY",
            "created_at": "2023-05-18 02:41:48.149307"
        },
        "final_report": {
            "accuracy": 0.2727272727272727
        }
    },
    "swedish-spelling.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "swedish-spelling.dev.v0",
            "base_eval": "swedish-spelling",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "swedish-spelling/samples.jsonl"
                    },
                    "key": "swedish-spelling.dev.v0",
                    "group": "swedish-spelling"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518051102QT7HTYFY",
            "created_at": "2023-05-18 05:11:02.705875"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "mmlu-clinical-knowledge.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-clinical-knowledge.val.ab-v1",
            "base_eval": "mmlu-clinical-knowledge",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=clinical_knowledge&split=validation"
                    },
                    "key": "mmlu-clinical-knowledge.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518023152MCZXCDKG",
            "created_at": "2023-05-18 02:31:52.090603"
        },
        "final_report": {
            "accuracy": 0.3
        }
    },
    "mmlu-conceptual-physics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-conceptual-physics.val.ab-v1",
            "base_eval": "mmlu-conceptual-physics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=conceptual_physics&split=validation"
                    },
                    "key": "mmlu-conceptual-physics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518025406KL6ZZ2L2",
            "created_at": "2023-05-18 02:54:06.504756"
        },
        "final_report": {
            "accuracy": 0.45
        }
    },
    "multi-step-equations.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "multi-step-equations.dev.v0",
            "base_eval": "multi-step-equations",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "multi-step-equations/samples.jsonl"
                    },
                    "key": "multi-step-equations.dev.v0",
                    "group": "multi-step-equations"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517212733BNCXUG4I",
            "created_at": "2023-05-17 21:27:33.508094"
        },
        "final_report": {
            "accuracy": 0.06666666666666667
        }
    },
    "mmlu-machine-learning.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-machine-learning.val.ab-v1",
            "base_eval": "mmlu-machine-learning",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=machine_learning&split=validation"
                    },
                    "key": "mmlu-machine-learning.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518041552STMLPESS",
            "created_at": "2023-05-18 04:15:52.860592"
        },
        "final_report": {
            "accuracy": 0.18181818181818182
        }
    },
    "mmlu-high-school-psychology.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-psychology.val.ab-v1",
            "base_eval": "mmlu-high-school-psychology",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_psychology&split=validation"
                    },
                    "key": "mmlu-high-school-psychology.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180346517MPJPRVP",
            "created_at": "2023-05-18 03:46:51.250399"
        },
        "final_report": {
            "accuracy": 0.8
        }
    },
    "partially_solved_crossword_clues.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "partially_solved_crossword_clues.dev.v0",
            "base_eval": "partially_solved_crossword_clues",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "partially_solved_crossword_clues/samples.jsonl"
                    },
                    "key": "partially_solved_crossword_clues.dev.v0",
                    "group": "partially_solved_crossword_clues"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517233630KIVUANMD",
            "created_at": "2023-05-17 23:36:30.660959"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "rap-people-vs-fruits.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "rap-people-vs-fruits.dev.v0",
            "base_eval": "rap-people-vs-fruits",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/rap_people_vs_fruits.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "rap-people-vs-fruits.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517221931BU4M3CT3",
            "created_at": "2023-05-17 22:19:31.066344"
        },
        "final_report": {
            "counts/No": 3,
            "counts/Yes": 6,
            "score": 0.6666666666666666
        }
    },
    "job_listing_title_for_a_caregiver_in_japan.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "job_listing_title_for_a_caregiver_in_japan.test.v1",
            "base_eval": "job_listing_title_for_a_caregiver_in_japan",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "job_listing_title_for_a_caregiver_in_japan/samples.jsonl"
                    },
                    "key": "job_listing_title_for_a_caregiver_in_japan.test.v1",
                    "group": "job_listing_title_for_a_caregiver_in_japan"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517203930THAU4K3H",
            "created_at": "2023-05-17 20:39:30.882542"
        },
        "final_report": {
            "accuracy": 0.5333333333333333
        }
    },
    "mmlu-high-school-european-history.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-high-school-european-history.val.ab-v1",
            "base_eval": "mmlu-high-school-european-history",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=high_school_european_history&split=validation"
                    },
                    "key": "mmlu-high-school-european-history.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "2305180322297GSAHBTA",
            "created_at": "2023-05-18 03:22:29.295443"
        },
        "final_report": {
            "accuracy": 0.3888888888888889
        }
    },
    "mmlu-management.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-management.val.ab-v1",
            "base_eval": "mmlu-management",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=management&split=validation"
                    },
                    "key": "mmlu-management.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518041811FNHFMBCQ",
            "created_at": "2023-05-18 04:18:11.912845"
        },
        "final_report": {
            "accuracy": 0.36363636363636365
        }
    },
    "emoji-riddle.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "emoji-riddle.s1.simple-v0",
            "base_eval": "emoji-riddle",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "emoji_riddle/fuzzy_match.jsonl"
                    },
                    "key": "emoji-riddle.s1.simple-v0",
                    "group": "emoji-riddle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518001757BNB3Q3Z3",
            "created_at": "2023-05-18 00:17:57.992262"
        },
        "final_report": {
            "accuracy": 0.15,
            "f1_score": 0.12788912579957357
        }
    },
    "mmlu-nutrition.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-nutrition.val.ab-v1",
            "base_eval": "mmlu-nutrition",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=nutrition&split=validation"
                    },
                    "key": "mmlu-nutrition.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518043120TK5SOKC6",
            "created_at": "2023-05-18 04:31:20.626838"
        },
        "final_report": {
            "accuracy": 0.55
        }
    },
    "stock-options-inverse-iron-butterfly-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-inverse-iron-butterfly-spread.dev.v0",
            "base_eval": "stock-options-inverse-iron-butterfly-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_inverse_iron_butterfly_spread.jsonl"
                    },
                    "key": "stock-options-inverse-iron-butterfly-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517213217UIEWFGLX",
            "created_at": "2023-05-17 21:32:17.786108"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "rap-people-vs-people.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "rap-people-vs-people.dev.v0",
            "base_eval": "rap-people-vs-people",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.modelgraded.classify:ModelBasedClassify",
                    "args": {
                        "samples_jsonl": "test_multiio/battles/rap_people_vs_people.jsonl",
                        "eval_type": "cot_classify",
                        "modelgraded_spec": "battle"
                    },
                    "key": "rap-people-vs-people.dev.v0",
                    "group": "test-modelgraded-battle"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517220444DWLA75HQ",
            "created_at": "2023-05-17 22:04:44.680973"
        },
        "final_report": {
            "counts/Yes": 8,
            "counts/__invalid__": 1,
            "score": 0.8888888888888888
        }
    },
    "hand_ranks.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "hand_ranks.test.v1",
            "base_eval": "hand_ranks",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "poker_hand_ranks/full_samples.jsonl"
                    },
                    "key": "hand_ranks.test.v1",
                    "group": "poker_hand_ranks"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517210107CGUJB6YA",
            "created_at": "2023-05-17 21:01:07.393866"
        },
        "final_report": {
            "accuracy": 0.1
        }
    },
    "moral_exceptQA.test.v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "moral_exceptQA.test.v1",
            "base_eval": "moral_exceptQA",
            "split": "test",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "moral_exceptQA/samples.jsonl"
                    },
                    "key": "moral_exceptQA.test.v1",
                    "group": "moral_exceptQA"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517232614OCZ2E5CP",
            "created_at": "2023-05-17 23:26:14.818922"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "invoices.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "invoices.dev.v0",
            "base_eval": "invoices",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "invoices/match.jsonl"
                    },
                    "key": "invoices.dev.v0",
                    "group": "invoices"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517211902L7KPXWMW",
            "created_at": "2023-05-17 21:19:02.153208"
        },
        "final_report": {
            "accuracy": 0.35
        }
    },
    "stock-option-terms-iron-butterfly-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-iron-butterfly-spread.dev.v0",
            "base_eval": "stock-option-terms-iron-butterfly-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_iron_butterfly_spread.jsonl"
                    },
                    "key": "stock-option-terms-iron-butterfly-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517214423SYPRV3HX",
            "created_at": "2023-05-17 21:44:23.020009"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "mmlu-world-religions.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-world-religions.val.ab-v1",
            "base_eval": "mmlu-world-religions",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=world_religions&split=validation"
                    },
                    "key": "mmlu-world-religions.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518050718IVDV2D2L",
            "created_at": "2023-05-18 05:07:18.571034"
        },
        "final_report": {
            "accuracy": 0.631578947368421
        }
    },
    "stock-options-bull-call-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-bull-call-spread.dev.v0",
            "base_eval": "stock-options-bull-call-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_bull_call_spread.jsonl"
                    },
                    "key": "stock-options-bull-call-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517213132QEBR4XMM",
            "created_at": "2023-05-17 21:31:32.802142"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "test-includes.s1.simple-v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "test-includes.s1.simple-v0",
            "base_eval": "test-includes",
            "split": "s1",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.includes:Includes",
                    "args": {
                        "samples_jsonl": "test_fuzzy_match/samples.jsonl"
                    },
                    "key": "test-includes.s1.simple-v0",
                    "group": "test-basic"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517215703SGS63DVT",
            "created_at": "2023-05-17 21:57:03.087045"
        },
        "final_report": {
            "accuracy": 0.6666666666666666
        }
    },
    "mmlu-professional-medicine.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-professional-medicine.val.ab-v1",
            "base_eval": "mmlu-professional-medicine",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=professional_medicine&split=validation"
                    },
                    "key": "mmlu-professional-medicine.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518044712INOLCW6F",
            "created_at": "2023-05-18 04:47:12.728182"
        },
        "final_report": {
            "accuracy": 0.4
        }
    },
    "stock-options-inverse-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-options-inverse-iron-condor-spread.dev.v0",
            "base_eval": "stock-options-inverse-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_options_inverse_iron_condor_spread.jsonl"
                    },
                    "key": "stock-options-inverse-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517214023LHK53LEC",
            "created_at": "2023-05-17 21:40:23.193375"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "first-letters.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "first-letters.dev.v0",
            "base_eval": "first-letters",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "first-letters/samples.jsonl"
                    },
                    "key": "first-letters.dev.v0",
                    "group": "first-letters"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517210142UGJC6374",
            "created_at": "2023-05-17 21:01:42.361455"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "mmlu-medical-genetics.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-medical-genetics.val.ab-v1",
            "base_eval": "mmlu-medical-genetics",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=medical_genetics&split=validation"
                    },
                    "key": "mmlu-medical-genetics.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518042211ID23QROR",
            "created_at": "2023-05-18 04:22:11.336703"
        },
        "final_report": {
            "accuracy": 0.9090909090909091
        }
    },
    "imperial_date_to_string.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "imperial_date_to_string.dev.v0",
            "base_eval": "imperial_date_to_string",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "imperial_date_to_string/samples.jsonl"
                    },
                    "key": "imperial_date_to_string.dev.v0",
                    "group": "imperial_date_to_string"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517225826PQYBMTDE",
            "created_at": "2023-05-17 22:58:26.996516"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "mmlu-jurisprudence.val.ab-v1.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "mmlu-jurisprudence.val.ab-v1",
            "base_eval": "mmlu-jurisprudence",
            "split": "val",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.multiple_choice:MultipleChoice",
                    "args": {
                        "dataset": "hf://cais/mmlu?name=jurisprudence&split=validation"
                    },
                    "key": "mmlu-jurisprudence.val.ab-v1",
                    "group": "mmlu"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518041024ZSG5OQ7U",
            "created_at": "2023-05-18 04:10:24.028159"
        },
        "final_report": {
            "accuracy": 0.5454545454545454
        }
    },
    "stock-option-terms-inverse-iron-condor-spread.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "stock-option-terms-inverse-iron-condor-spread.dev.v0",
            "base_eval": "stock-option-terms-inverse-iron-condor-spread",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "stock_options/stock_option_terms_inverse_iron_condor_spread.jsonl"
                    },
                    "key": "stock-option-terms-inverse-iron-condor-spread.dev.v0",
                    "group": "stock-options"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517214647OAO446IG",
            "created_at": "2023-05-17 21:46:47.854433"
        },
        "final_report": {
            "accuracy": 0.25
        }
    },
    "convert-hex-hsl-lightness.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "convert-hex-hsl-lightness.dev.v0",
            "base_eval": "convert-hex-hsl-lightness",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "convert-hex-hsl-lightness/samples.jsonl"
                    },
                    "key": "convert-hex-hsl-lightness.dev.v0",
                    "group": "convert-hex-hsl-lightness"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230518000802YKHS7VHH",
            "created_at": "2023-05-18 00:08:02.494260"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "map-electronic-component-part-to-fact.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "map-electronic-component-part-to-fact.dev.v0",
            "base_eval": "map-electronic-component-part-to-fact",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "map-electronic-component-part-to-fact/samples.jsonl"
                    },
                    "key": "map-electronic-component-part-to-fact.dev.v0",
                    "group": "map-electronic-component-part-to-fact"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "230517223921UFXR27SS",
            "created_at": "2023-05-17 22:39:21.379256"
        },
        "final_report": {
            "accuracy": 0.0
        }
    },
    "logic-statements.dev.v0.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "logic-statements.dev.v0",
            "base_eval": "logic-statements",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.fuzzy_match:FuzzyMatch",
                    "args": {
                        "samples_jsonl": "logic-statements/logic-statements.jsonl"
                    },
                    "key": "logic-statements.dev.v0",
                    "group": "logic-statements"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23051800184757VGIQ56",
            "created_at": "2023-05-18 00:18:47.652557"
        },
        "final_report": {
            "accuracy": 0.4,
            "f1_score": 0.05
        }
    },
    "crepe.dev.v2.json": {
        "spec": {
            "completion_fns": [
                "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
            ],
            "eval_name": "crepe.dev.v2",
            "base_eval": "crepe",
            "split": "dev",
            "run_config": {
                "completion_fns": [
                    "OpenAssistant/oasst-rlhf-3-llama-30b-5k-steps"
                ],
                "eval_spec": {
                    "cls": "evals.elsuite.basic.match:Match",
                    "args": {
                        "samples_jsonl": "crepe/samples.jsonl"
                    },
                    "key": "crepe.dev.v2",
                    "group": "crepe"
                },
                "seed": 20220722,
                "max_samples": null,
                "command": "./main.py",
                "initial_settings": {
                    "visible": true
                }
            },
            "created_by": "",
            "run_id": "23051801200134HUTBXH",
            "created_at": "2023-05-18 01:20:01.430928"
        },
        "final_report": {
            "accuracy": 0.1
        }
    }
}