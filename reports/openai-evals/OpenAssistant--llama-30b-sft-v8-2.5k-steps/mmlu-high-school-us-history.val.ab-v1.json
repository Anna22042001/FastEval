{"spec": {"completion_fns": ["open-assistant:OpenAssistant/llama-30b-sft-v8-2.5k-steps"], "eval_name": "mmlu-high-school-us-history.val.ab-v1", "base_eval": "mmlu-high-school-us-history", "split": "val", "run_config": {"completion_fns": ["open-assistant:OpenAssistant/llama-30b-sft-v8-2.5k-steps"], "eval_spec": {"cls": "evals.elsuite.multiple_choice:MultipleChoice", "args": {"dataset": "hf://cais/mmlu?name=high_school_us_history&split=validation"}, "key": "mmlu-high-school-us-history.val.ab-v1", "group": "mmlu"}, "seed": 20220722, "max_samples": null, "command": "./evaluate.py -e -m open-assistant:OpenAssistant/llama-30b-sft-v8-2.5k-steps -b openai-evals", "initial_settings": {"visible": true}}, "created_by": "", "run_id": "2306181815233RN6LYWF", "created_at": "2023-06-18 18:15:23.685350"}}
{"final_report": {"accuracy": 0.45}}
