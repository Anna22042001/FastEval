{
    "score": 0.1,
    "model_outputs": [
        {
            "id": 0,
            "question": "Which one of the following is the most appropriate definition of a 99% confidence interval?\n\n(A) 99% of the time in repeated samples, the interval would contain the true value of the parameter\n(B) 99% of the time in repeated samples, the interval would contain the estimated value of the parameter\n(C) 99% of the time in repeated samples, the null hypothesis will be rejected\n(D) 99% of the time in repeated samples, the null hypothesis will not be rejected when it was false",
            "correct_answer": 0,
            "model_answer": "Option A. \n\nA 99% confidence interval is a range of values that 99% of the time in repeated samples, would contain the true value of the parameter, not the estimated value. This means that if we perform the same study over and over again, 99% of the time the confidence interval will include the true value of the population parameter.",
            "correct": false
        },
        {
            "id": 1,
            "question": "What is the main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing?\n\n(A) ADF is a single equation approach to unit root testing while PP is a systems approach\n(B) PP tests reverse the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PP test\n(C) The PP test incorporates an automatic correction for autocorrelated residuals in the test regression\n(D) PP tests have good power in small samples whereas DF tests do not.",
            "correct_answer": 2,
            "model_answer": "The main difference between the Dickey Fuller (DF) and Phillips-Perron (PP) approaches to unit root testing is option (B). The PP test reverses the DF null and alternative hypotheses so that there is stationarity under the null hypothesis of the PF test. In the DF test, a unit root is rejected if the coefficient on the lagged variable is significantly different from zero. In contrast, the PP test assumes that the time series is stationary under the null hypothesis, and a unit root is rejected if the residuals from an augmented Dickey-Fuller regression are stationary and have mean zero. This makes the PP test more focused on the distribution of the residuals rather than the coefficient on the lagged variable.",
            "correct": false
        },
        {
            "id": 2,
            "question": "If there were a leverage effect in practice, what would be the shape of the news impact curve for as model that accounted for that leverage?\n\n(A) It would rise more quickly for negative disturbances than for positive ones of the same magnitude\n(B) It would be symmetrical about zero\n(C) It would rise less quickly for negative disturbances than for positive ones of the same magnitude\n(D) It would be zero for all positive disturbances",
            "correct_answer": 0,
            "model_answer": "The leverage effect refers to the idea that news about negative events has a greater impact on stock prices than news about positive events. This is because negative news often leads to a greater perceived downside risk, which causes investors to sell off stocks in anticipation of further losses.\nIf we were to model the news impact curve for a leverage effect, we would likely see that it rises more quickly for negative disturbances than for positive ones of the same magnitude. This is because negative news has a greater impact on investor sentiment and is more likely to lead to selling pressure in the market.\nTherefore, the answer is:\n(A) It would rise more quickly for negative disturbances than for positive ones of the same magnitude.",
            "correct": true
        },
        {
            "id": 3,
            "question": "Which of the following statements is false concerning the linear probability model?\n\n(A) There is nothing in the model to ensure that the estimated probabilities lie between zero and one\n(B) Even if the probabilities are truncated at zero and one, there will probably be many observations for which the probability is either exactly zero or exactly one\n(C) The error terms will be heteroscedastic and not normally distributed\n(D) The model is much harder to estimate than a standard regression model with a continuous dependent variable",
            "correct_answer": 3,
            "model_answer": "The false statement is (A) There is nothing in the model to ensure that the estimated probabilities lie between zero and one.\n\nLinear probability model is also known as logit model. The logit model has probabilities that are linear in the parameters and the link function used is the inverse logistic function or logit function. The logit function maps the values between zero and one by converting them to a value between negative infinity and zero.\n\nThe logit function is given by:\n\nlogit(p) = log(p/(1-p))\n\nThis ensures that the estimated probabilities lie between zero and one.",
            "correct": false
        },
        {
            "id": 4,
            "question": "Which of the following statements concerning the regression population and sample is FALSE?\n\n(A) The population is the total collection of all items of interest\n(B) The population can be infinite\n(C) In theory, the sample could be larger than the population\n(D) A random sample is one where each individual item from the population is equally likely to be drawn.",
            "correct_answer": 2,
            "model_answer": "(D) A random sample is one where each individual item from the population is equally likely to be drawn.\n\nThe reason is that this statement is False. The other statements are True.",
            "correct": false
        },
        {
            "id": 5,
            "question": "Which of the following statements is INCORRECT concerning a comparison of the Box-Pierce Q and the Ljung-Box Q* statistics for linear dependence in time series?\n\n(A) Asymptotically, the values of the two test statistics will be equal\n(B) The Q test has better small-sample properties than the Q*\n(C) The Q test is sometimes over-sized for small samples\n(D) As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients.",
            "correct_answer": 1,
            "model_answer": "(D) As the sample size tends towards infinity, both tests will show a tendency to always reject the null hypothesis of zero autocorrelation coefficients.\n\nThis statement is incorrect. \n\nThe Box-Pierce Q and the Ljung-Box Q* statistics are both used to test for linear dependence in time series. However, their tendencies to reject the null hypothesis of zero autocorrelation coefficients differ as the sample size increases.\n\nThe Box-Pierce Q test is designed to have a conservative (lower) level of significance, meaning that it only has a tendency to reject the null hypothesis when there is indeed linear dependence in the time series. As the sample size increases, the probability of the Q test incorrectly rejecting the null hypothesis (Type I error) decreases.\n\nOn the other hand, the Ljung-Box Q* test has a tendency to \"over-reject\" the null hypothesis, meaning that it has a higher (upper) level of significance. As the sample size increases, the probability of the Q* test incorrectly rejecting the null hypothesis (Type I error) decreases, but the probability of the test failing to reject the null hypothesis when there is indeed linear dependence (Type II error) increases.\n\nSo, while both tests have a tendency to reject the null hypothesis as the sample size increases, the Box-Pierce Q test is more conservative and has a lower level of significance, making it more suitable for testing for linear dependence in time series.",
            "correct": false
        },
        {
            "id": 6,
            "question": "A parsimonious model is one that\n\n(A) Includes too many variables\n(B) Includes as few variables as possible to explain the data\n(C) Is a well-specified model\n(D) Is a mis-specified model",
            "correct_answer": 0,
            "model_answer": "The correct answer is (B) Includes as few variables as possible to explain the data.\n\nA parsimonious model is one that aims to explain the data using the minimum number of variables necessary. This approach is preferred because it reduces the complexity of the model and makes it easier to interpret and understand. By including as few variables as possible, the model is more likely to accurately represent the underlying relationships between the variables and the observed data. This results in a simpler, more accurate, and more reliable model.",
            "correct": false
        },
        {
            "id": 7,
            "question": "Which of the following is NOT a feature of continuously compounded returns (i.e. log-returns)?\n\n(A) They can be interpreted as continuously compounded changes in the prices\n(B) They can be added over time to give returns for longer time periods\n(C) They can be added across a portfolio of assets to give portfolio returns\n(D) They are usually fat-tailed",
            "correct_answer": 2,
            "model_answer": "The answer is (D) They are usually fat-tailed. \n\nA log-return is the natural logarithm of the price at time t divided by the price at time t-1. It measures the continuously compounded return over the period [t-1, t]. The feature that makes log-returns unique is that they are obtained by taking the logarithm of the price changes, which means that they incorporate the effects of compounding directly. \n\n(A) They can be interpreted as continuously compounded changes in the prices - This is a feature of log-returns as they measure the continuously compounded returns. \n(B) They can be added over time to give returns for longer time periods- This is also a feature of log-returns, as they can be added to give returns for longer time periods. \n(C) They can be added across a portfolio of assets to give portfolio returns - This is also a feature of log-returns, as they can be added across a portfolio of assets to give the portfolio returns. \n\nHowever, (D) They are usually fat-tailed is not a feature of log-returns. Fat tails refer to the asymmetry in the distribution of financial returns, where the tail of the distribution extends far out into the right. Log-returns, on the other hand, are normally distributed, meaning that their tails are not fat. The reasoning behind this is that although financial returns may be fat-tailed, taking the logarithm of the returns reduces their fat-tailedness, as log-normal distributions are less sensitive to outliers compared to normal distributions. \n\nTherefore, (D) They are usually fat-tailed is not a feature of continuously compounded returns (i.e. log-returns). \n\nFinal answer: (D) They are usually fat-tailed",
            "correct": false
        },
        {
            "id": 8,
            "question": "Which of the following features of financial asset return time-series could be captured using a standard GARCH(1,1) model?\n\ni) Fat tails in the return distribution\n\n\nii) Leverage effects\n\n\niii) Volatility clustering\n\n\niv) Volatility affecting returns\n\n(A) (ii) and (iv) only\n(B) (i) and (iii) only\n(C) (i), (ii), and (iii) only\n(D) (i), (ii), (iii), and (iv)",
            "correct_answer": 1,
            "model_answer": "The GARCH(1,1) model is a popular model used to model the conditional heteroscedasticity in financial asset return time-series. It captures the past volatility in predicting current volatility. Let's examine each feature mentioned and see which ones can be captured by a GARCH(1,1) model:\n\n1) Fat tails in the return distribution: The GARCH(1,1) model assumes that the return distribution is normal, which means it does not capture fat tails. However, some extensions of the GARCH model, like the Student-t GARCH model, can capture fat tails. So, the answer is (A) (ii) and (iv) only.\n\n2) Leverage effects: The GARCH(1,1) model can capture leverage effects as it allows for volatility to be impacted by previous volatility. In other words, high volatility in the past can lead to higher expected volatility in the future. So, the answer is (A) (ii) and (iv) only.\n\n3) Volatility clustering: The GARCH(1,1) model assumes that volatility follows an AR(1) process, which means that volatility can have persistent effects. In other words, high volatility in the past can lead to higher volatility in the future. This means that the GARCH(1,1) model can capture volatility clustering to some extent. So, the answer is (C) (i), (ii), and (iii) only.\n\n4) Volatility affecting returns: The GARCH(1,1) model assumes that returns and volatility are linearly related, which means that an increase in volatility will lead to an increase in the conditional mean of returns. So, the answer is (A) (ii) and (iv) only.\n\nIn conclusion, the GARCH(1,1) model can capture leverage effects and volatility affecting returns, while it cannot capture fat tails and volatility clustering. The correct answer is (A) (ii) and (iv) only.",
            "correct": false
        },
        {
            "id": 9,
            "question": "Consider the estimation of a GARCH-M model. If the data employed were a time-series of daily corporate bond percentage returns, which of the following would you expect the value of the GARCH-in-mean parameter estimate to be?\n\n(A) Less than -1\n(B) Between -1 and 0\n(C) Between 0 and 1\n(D) Bigger than 1",
            "correct_answer": 2,
            "model_answer": "To determine the expected value of the GARCH-in-mean parameter estimate for a GARCH-M model applied to a time-series of daily corporate bond percentage returns, we need to consider the properties of corporate bond returns.\n\nGARCH-M models assume that the variance of the returns is caused by the mean of the returns in the previous period. In the context of corporate bond returns, the mean return could reflect the credit quality of the bond issuer, macroeconomic factors affecting the bond market, and other factors specific to the bond issue.\n\nIf the corporate bond returns exhibit negative skewness, it is likely that the mean return will be lower than the average return of the time-series. This is because negative events such as default or credit rating downgrades have a higher probability of occurring than positive events such as an upgrade or recovery.\n\nGiven the negative skewness of corporate bond returns, we would expect the GARCH-in-mean parameter estimate to be less than -1. This means that the variance of the returns increases if the mean return is negative, which reflects the higher probability of negative returns in the future.\n\nTherefore, the expected value of the GARCH-in-mean parameter estimate for a GARCH-M model applied to a time-series of daily corporate bond percentage returns would be:\n\n(A) Less than -1",
            "correct": false
        }
    ]
}