{
    "score": 0.425,
    "model_outputs": [
        {
            "id": 17,
            "question": "For a neural network, which one of these structural assumptions is the one that most affects the trade-off between underfitting (i.e. a high bias model) and overfitting (i.e. a high variance model):\n\n(A) The number of hidden nodes\n(B) The learning rate\n(C) The initial choice of weights\n(D) The use of a constant-term unit input",
            "correct_answer": 0,
            "model_answer": "1. The number of hidden nodes affects the model's ability to learn complex features and patterns in the data.\n2. The learning rate affects the model's ability to converge to a solution.\n3. The initial choice of weights affects the model's ability to learn a good starting point for the weights.\n4. The use of a constant-term unit input affects the model's ability to learn non-linear relationships between the inputs and outputs.\n\nThe final answer is (D). The use of a constant-term unit input can lead to overfitting and a high variance model.",
            "correct": false
        },
        {
            "id": 72,
            "question": "Statement 1| Industrial-scale neural networks are normally trained on CPUs, not GPUs. Statement 2| The ResNet-50 model has over 1 billion parameters.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "The ResNet-50 model has over 1 billion parameters.",
            "correct": 0.25
        },
        {
            "id": 108,
            "question": "Which of the following is false?\n\n(A) Semantic segmentation models predict the class of each pixel, while multiclass image classifiers predict the class of entire image.\n(B) A bounding box with an IoU (intersection over union) equal to $96\\%$ would likely be considered at true positive.\n(C) When a predicted bounding box does not correspond to any object in the scene, it is considered a false positive.\n(D) A bounding box with an IoU (intersection over union) equal to $3\\%$ would likely be considered at false negative.",
            "correct_answer": 3,
            "model_answer": "The answer is (B). A bounding box with an IoU equal to $96\\%$ would likely be considered at true positive.",
            "correct": false
        },
        {
            "id": 102,
            "question": "You are training a linear regression model for a simple estimation task, and notice that the model is overfitting to the data. You decide to add in $\\ell_2$ regularization to penalize the weights. As you increase the $\\ell_2$ regularization coefficient, what will happen to the bias and variance of the model?\n\n(A) Bias increase ; Variance increase\n(B) Bias increase ; Variance decrease\n(C) Bias decrease ; Variance increase\n(D) Bias decrease ; Variance decrease",
            "correct_answer": 1,
            "model_answer": "(A)",
            "correct": false
        },
        {
            "id": 97,
            "question": "Which one of the following is equal to P(A, B, C) given Boolean random variables A, B and C, and no independence or conditional independence assumptions between any of them?\n\n(A) P(A | B) * P(B | C) * P(C | A)\n(B) P(C | A, B) * P(A) * P(B)\n(C) P(A, B | C) * P(C)\n(D) P(A | B, C) * P(B | A, C) * P(C | A, B)",
            "correct_answer": 2,
            "model_answer": "P(A, B, C) is equal to P(A, B, C) given Boolean random variables A, B and C, and no independence or conditional independence assumptions between any of them. \n\nExplanation:\n\n1. P(A) is the probability of A being true.\n2. P(B) is the probability of B being true.\n3. P(C) is the probability of C being true.\n4. P(A, B) is the probability of A and B being true.\n5. P(B, C) is the probability of B and C being true.\n6. P(C, A) is the probability of C and A being true.\n7. P(A, B, C) is the product of all the probabilities.\n\nTherefore, P(A, B, C) is equal to P(A, B, C) given Boolean random variables A, B and C, and no independence or conditional independence assumptions between any of them.",
            "correct": true
        },
        {
            "id": 8,
            "question": "Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?\n\n(A) 2\n(B) 4\n(C) 8\n(D) 16",
            "correct_answer": 2,
            "model_answer": "2\n\nStep 1: Identify the variables in the Bayesian network.\nVariables: H, U, P, W.\n\nStep 2: Determine the number of independent parameters needed.\nSince the variables are mutually exclusive, we can determine the number of independent parameters by subtracting the number of parameters that are not independent.\n\nStep 3: Count the number of parameters that are not independent.\nThe number of parameters that are not independent is the sum of the number of parameters that are mutually exclusive.\n\nStep 4: Determine the number of independent parameters needed.\nThe number of independent parameters needed is the difference between the number of parameters that are not independent and the number of parameters that are mutually exclusive.\n\nStep 5: Apply the formula.\nThe number of independent parameters needed is 2.",
            "correct": false
        },
        {
            "id": 32,
            "question": "Statement 1| After mapped into feature space Q through a radial basis kernel function, 1-NN using unweighted Euclidean distance may be able to achieve better classification performance than in original space (though we can\u2019t guarantee this). Statement 2| The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "The final answer is D. The reasoning step-by-step:\n\n1. The statement 1 is true.\n2. The statement 2 is true.\n3. Therefore, the final answer is D.",
            "correct": false
        },
        {
            "id": 15,
            "question": "Statement 1| Layer Normalization is used in the original ResNet paper, not Batch Normalization. Statement 2| DCGANs use self-attention to stabilize training.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 1,
            "model_answer": "The final line should contain the answer as a single letter with parenthesis without anything else. The answer is 'B'.",
            "correct": true
        },
        {
            "id": 63,
            "question": "What are support vectors?\n\n(A) The examples farthest from the decision boundary.\n(B) The only examples necessary to compute f(x) in an SVM.\n(C) The data centroid.\n(D) All the examples that have a non-zero weight \u03b1k in a SVM.",
            "correct_answer": 1,
            "model_answer": "The answer is (B). Support vectors are the data points that are closest to the decision boundary, and are used to compute the SVM model.",
            "correct": true
        },
        {
            "id": 57,
            "question": "Statement 1| L2 regularization of linear models tends to make models more sparse than L1 regularization. Statement 2| Residual connections can be found in ResNets and Transformers.\n\n(A) True, True\n(B) False, False\n(C) True, False\n(D) False, True",
            "correct_answer": 3,
            "model_answer": "The final line should contain the answer as a single letter with parenthesis without anything else. The answer is 'D'.",
            "correct": true
        }
    ],
    "num_unmatched": 1
}